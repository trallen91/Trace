{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import string\n",
    "\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nb Formatting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_seq_items = 2000\n",
    "\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ignore header for easier (ordinal) indexing\n",
    "datasheet = pd.read_excel('CogData_FU_82818.xlsx',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(datasheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasheet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Row Indices of Memory Responses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ignore first (header) column\n",
    "memory_ix = datasheet.iloc[1:][datasheet.iloc[1:,2].notnull()].index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record IDs of Memory Respondents [Analyze all scores from each participant]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "memory_id = list(datasheet.iloc[memory_ix][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm Unique Response-Respondents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(memory_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(set(memory_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasheet.iloc[memory_ix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tech respondents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_ix = datasheet.iloc[1:][datasheet.iloc[1:,3].notnull()].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_id = list(datasheet.iloc[tech_ix][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(tech_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#each respondent responded to one prompt\n",
    "len(tech_id) == len(set(tech_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ix_id_series = pd.Series(tech_id,index=tech_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ix_id_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tech-exclusive respondents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#row indices of tech-respondents, mem-negligent\n",
    "tech_x_ix = [tix for tix in tech_ix if tix not in memory_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#record ids of tech-respondents who are mem-negligent\n",
    "tech_x_ids = [tid for tid in tech_id if tid not in memory_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mem-exclusive respondents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#mem respondents who are tech-negligent\n",
    "[m for m in memory_id if m not in tech_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indices of all Respondent-rows, by record ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#all rows for tech_respondents\n",
    "tech_all_ix = [ix for ix in datasheet.index if datasheet.iloc[ix,0] in tech_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max(datasheet[datasheet[0]==14].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasheet.iloc[tech_all_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem_all_ix = [ix for ix in datasheet.index if datasheet.iloc[ix,0] in memory_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasheet.iloc[mem_all_ix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column index Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate score spread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasheet.iloc[tech_ix,51:54]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#MMSE\n",
    "datasheet.iloc[tech_all_ix][[0,1,4,74]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MMSE scores\n",
    "#6 unique...\n",
    "np.unique([int(i) for i in datasheet.iloc[tech_all_ix][74].dropna()],return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Min MMSE\n",
    "datasheet.iloc[tech_all_ix][74].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The maximum MMSE score is 30 points. A score of 20 to 24 suggests mild dementia, 13 to 20 suggests moderate dementia, and less than 12 indicates severe dementia.\n",
    "#participants with MCI...one ---\n",
    "datasheet.iloc[tech_all_ix][74][datasheet.iloc[tech_all_ix][74] < 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(tech_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lancet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lancet = pd.read_excel('Lancet_82318.xlsx',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[(i,name) for (i,name) in enumerate(lancet.iloc[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lancet_id = set(lancet[0][1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#lancet dataset accounts for all respondents \n",
    "[no_lance for no_lance in tech_id if no_lance not in lancet_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lancet.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#indices in lancet dataset for which record_id responded to either/both prompts\n",
    "lancet_all_ix = [ix for ix in lancet.index if lancet.loc[ix][0] in tech_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lancet_id_ix = lancet.iloc[lancet_all_ix,[0,17]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#each participant has one diagnosis row, other rows are na --> retain one diagnosis per patient \n",
    "lancet_id_ix = lancet_id_ix.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataset breakdown \n",
    "lancet_id_ix[17].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(lancet_id_ix) == len(ix_id_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(np.where(lancet.iloc[lancet_all_ix][2] == 'Female')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(np.where(lancet.iloc[lancet_all_ix][2] == 'Male')[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lancet.iloc[lancet_all_ix][5].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean(lancet.iloc[lancet_all_ix][5].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max(lancet.iloc[lancet_all_ix][5].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min(lancet.iloc[lancet_all_ix][5].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lancet.iloc[520:530]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lancet.iloc[lancet_all_ix][[16,17]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.where(min(lancet.iloc[lancet_all_ix][5].dropna()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasheet_col_ix_name = [(i,test) for( i,test) in enumerate(datasheet.iloc[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 466k Word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://raw.githubusercontent.com/dwyl/english-words/master/words.txt\n",
    "with open('words.txt','r') as words:\n",
    "    word_list = words.read().splitlines()\n",
    "\n",
    "word_list = [w.lower() for w in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nltk pos data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#memory response\n",
    "mem_corpus = datasheet.iloc[memory_ix,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tech response\n",
    "tech_corpus = datasheet.iloc[tech_ix,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try-catch comprehension hack\n",
    "https://stackoverflow.com/questions/1528237/how-can-i-handle-exceptions-in-a-list-comprehension-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def catch(func, handle=lambda e : e, *args, **kwargs):\n",
    "    try:\n",
    "        return func(*args, **kwargs)\n",
    "    \n",
    "    except ZeroDivisionError as z:\n",
    "        return None\n",
    "    except statistics.StatisticsError:\n",
    "        print('Stat error at ix')\n",
    "        return handle(e)\n",
    "    except Exception as e:\n",
    "        print(handle(e))\n",
    "        return None\n",
    "        #return handle(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Mis-spellings, word length\n",
    "#rm puctuation\n",
    "#keep contractions, posessives\n",
    "def list_tokenize(response):\n",
    "    return re.sub('['+string.punctuation.replace('\\'','')+']',' ',response).split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information extraction nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#information-extraction\n",
    "#https://www.nltk.org/book/ch07.html\n",
    "def ie_preprocess(document):\n",
    "    #sentence segmentation\n",
    "    sentences = nltk.sent_tokenize(document) \n",
    "    #tokenization\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    #pos tagging\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences] \n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniq, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uniq_words (response,token_list=None, counts_vector_return=False,uniq_set_return=False):\n",
    "    if token_list is None: token_list = list_tokenize(response)\n",
    "    if counts_vector_return and uniq_set_return: return np.unique(token_list,return_counts=True)\n",
    "    \n",
    "    if counts_vector_return and not uniq_set_return: return np.unique(token_list,return_counts=True)[1]\n",
    "    \n",
    "    if uniq_set_return and not counts_vector_return: return np.unique(token_list)\n",
    "    #typical case, investigating number of uniq responses\n",
    "    \n",
    "    return len(np.unique(token_list))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def single_appearances (response,counts=None):\n",
    "    if counts is None: counts = uniq_words(response,counts_vector_return=True)\n",
    "    return len(np.where(counts==1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://locallyoptimal.com/blog/2013/01/20/elegant-n-gram-generation-in-python/\n",
    "#input\n",
    "def find_bigrams(response, return_list=False):\n",
    "    input_list = list_tokenize(response)\n",
    "    bigrams = zip(input_list, input_list[1:])\n",
    "    if return_list: return [b for b in bigrams]\n",
    "    else: return bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_count(response):\n",
    "    return len(list_tokenize(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_count(response):\n",
    "    return len(ie_preprocess(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#information-extraction\n",
    "#https://www.nltk.org/book/ch07.html\n",
    "def ie_preprocess(document):\n",
    "    #sentence segmentation\n",
    "    sentences = nltk.sent_tokenize(document) \n",
    "    #tokenization\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    #pos tagging\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences] \n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this method is error-prone\n",
    "def pos_rate(response,pos_response=None,tag_set=None,denom_set=None,N=None,nn=False,pn=False,vb=False,cloc=False):\n",
    "    \n",
    "    if pos_response is None: pos_response = ie_preprocess(response)\n",
    "        \n",
    "    \n",
    "    if tag_set is None:\n",
    "        #common nouns, excluding proper nouns\n",
    "        if nn: tag_set = ['NN','NNS']\n",
    "        elif vb: tag_set = ['VB','VBD','VBG','VBN','VBP','VBZ'] \n",
    "        elif pn: tag_set = ['PRP','PRP$']\n",
    "            #engineer: include all specific nouns, unspecified nouns 'stuff' 'things' 'this' in denom -- list better than intuition?\n",
    "            #consider removing self-reference pronouns or count separately?\n",
    "        elif cloc: \n",
    "            tag_set = ['PRP','PRP$','WP', 'DT','WDT','WP$']\n",
    "            #amend-?\n",
    "#             thing_set = ['thing','things','stuff','anything','everything','something']\n",
    "            denom_set = ['NN','NNP','NNPS','NNS']\n",
    "#         elif: lexical_density = [] n adj vb advb \n",
    "        else: \n",
    "            print('Usage: set pos rate metric to True')\n",
    "            return None\n",
    "        \n",
    "    pos_count = sum([1 for sentence in pos_response for word in sentence if word[1] in tag_set])\n",
    "    \n",
    "    if denom_set: \n",
    "#         print('Thing_set',[word[0] for sentence in pos_response for word in sentence if word[0] in thing_set])\n",
    "#         print('Pronouns',[word[0] for sentence in pos_response for word in sentence if word[1] in tag_set])\n",
    "#         pos_count += sum([1 for sentence in pos_response for word in sentence if word[0] in thing_set])\n",
    "        denom_count = sum([1 for sentence in pos_response for word in sentence if word[1] in denom_set])\n",
    "        if denom_count == 0: return None\n",
    "    \n",
    "    else:\n",
    "        if N is None: N = word_count(response)\n",
    "        denom_count = N\n",
    "        \n",
    "    return pos_count/denom_count\n",
    "        \n",
    "\n",
    "#         sum([1 for sentence in pos_test for word in sentence if word[1] in ['VB','VBD','VBG','VBN','VBP','VBZ'] ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasheet.iloc[522,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function-word rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.ensure_loaded()\n",
    "swords = stopwords.words()\n",
    "\n",
    "def func_rate(response, words=swords):\n",
    "    response = list_tokenize(response)\n",
    "    N = len(response)\n",
    "#     print([word for word in response if word in swords])\n",
    "    func_count = sum([1 for word in response if word in swords])\n",
    "    return func_count/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "func_rate(datasheet.iloc[memory_ix[8],2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Honore's R \n",
    "![image.png](attachment:image.png)\n",
    "https://books.google.com/books?id=CwC4CwAAQBAJ&pg=PA111&lpg=PA111&dq=honore+statistic+lexical+richness&source=bl&ots=LPnM2j9oX5&sig=HGObkoMYRy6lLsgc80Y7tUMT7vk&hl=en&sa=X&ved=2ahUKEwjv0ubE7vTdAhXwYd8KHVRfCiMQ6AEwCXoECAUQAQ#v=onepage&q=honore&f=false\n",
    "\n",
    "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.2.1359&rep=rep1&type=pdf\n",
    "\n",
    "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.484.4648&rep=rep1&type=pdf\n",
    "\n",
    "viii) Honoré’s R statistic.\n",
    "If V1 denotes the number of hapax legomena recorded, then Honoré (1979) defined his\n",
    "statistic as:\n",
    "\n",
    "*R = 100log(N)/(1 - V1/V)*\n",
    "\n",
    "As a measure of vocabulary richness which has the virtue of being insensitive to text\n",
    "length, it has been used in stylometric studies by Holmes (1992) and Holmes and\n",
    "Stylometric analysis of conversational speech\n",
    "Forsyth (1995). Values of R typically range from 1000 to 2000 with higher values\n",
    "implying richer vocabularies in the sense that a greater number of words appear\n",
    "infrequently.\n",
    "\n",
    "\n",
    "https://pdfs.semanticscholar.org/11f9/ef33ad001f7638ba29ee8109077de92eb1bb.pdf\n",
    "\n",
    "\n",
    "Assuming log == base e...fits with 1000-2000 assumption\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def honore_r(response,v=None,v1=None, N=None):\n",
    "    if N is None: N = word_count(response)\n",
    "    if v is None: v = uniq_words(response)\n",
    "    if v1 is None: \n",
    "        counts = uniq_words(response,counts_vector_return=True)\n",
    "        v1 = len(np.where(counts==1)[0])\n",
    "    return (100*math.log(N))/(1-(v1/v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mem_honore = pd.Series([catch(lambda:honore_r(response)) for response in datasheet.iloc[memory_ix,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tech_honore = pd.Series([catch(lambda:honore_r(response)) for response in datasheet.iloc[tech_ix,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem_honore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brunet's W\n",
    "W = N**(V −0.165)\n",
    "\n",
    "(vii) Brunet’s W index.\n",
    "This index, devised by Brunet (1978) and used successfully by Holmes and Forsyth\n",
    "(1995), is a measure of vocabulary richness which is insensitive to text length. It is\n",
    "defined as:\n",
    "\n",
    "W= N^(V-0.165)\n",
    "where N is the text length, V the number of different words and (-0.165) is a scaling\n",
    "constant proposed by Brunet. The measure generally varies between 10 and 20 with a\n",
    "low value indicating a lexically richer speech.\n",
    "https://pdfs.semanticscholar.org/11f9/ef33ad001f7638ba29ee8109077de92eb1bb.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def brunet_w(response,N=None, v=None):\n",
    "    if N is None: N = word_count(response)\n",
    "    if v is None: v = uniq_words(response)\n",
    "    return N**(v-0.165)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#10-window size generator\n",
    "#https://docs.python.org/release/2.3.5/lib/itertools-example.html\n",
    "def window(seq, n=10):\n",
    "    \"Returns a sliding window (of width n) over data from the iterable\"\n",
    "    \"   s -> (s0,s1,...s[n-1]), (s1,s2,...,sn), ...                   \"\n",
    "    it = iter(seq)\n",
    "    result = tuple(islice(it, n))\n",
    "    if len(result) == n:\n",
    "        yield result    \n",
    "    for elem in it:\n",
    "        result = result[1:] + (elem,)\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[w for w in window(list_tokenize(datasheet.iloc[496,2]),8)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indices to pull other target-options in clinic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#indices to grab latest cog scores\n",
    "ix_score_latest = [max(datasheet[datasheet[0]==ix].index) for ix in tech_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ix_score_latest_completed = []\n",
    "for ix in tech_id:\n",
    "    slice_ix = list(datasheet[datasheet[0]==ix].index)\n",
    "    while datasheet.iloc[max(slice_ix)][96] != 'Complete':\n",
    "        slice_ix.remove(max(slice_ix))\n",
    "    ix_score_latest_completed += [max(slice_ix)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Held-out test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.random.choice(tech_ix,(1,int(.20*len(tech_ix))),replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "array([[ 29, 464, 324, 277, 242, 496, 107, 359, 411, 375]], dtype=int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "held_out_test_ix = np.array([ 29, 464, 324, 277, 242, 496, 107, 359, 411, 375])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "held_out_test_id = [i for i in datasheet.iloc[held_out_test_ix][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#held out test-set coincidently has both tech + mem responses\n",
    "#dev set has one tech_x responose\n",
    "tech_x_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_held_out_test_ix = [yt for yt in ix_score_latest_completed if datasheet.iloc[yt,0] in held_out_test_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.random.choice(tech_ix_train_dev,(1,int(.20*len(tech_ix_train_dev))),replace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_dev_ix = [t for t in tech_ix if t not in held_out_test_ix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "tech_ix_dev = np.random.choice(tech_ix_train_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_ix = np.array([460,  78, 234, 352, 347, 398, 448, 537])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_id = [i for i in datasheet.iloc[dev_ix][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_dev_ix = [yd for yd in ix_score_latest_completed if datasheet.iloc[yd,0] in dev_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ix = [i for i in train_dev_ix if i not in dev_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_id = [i for i in datasheet.iloc[train_ix][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(train_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ix_mem = [m for m in train_ix if m in memory_ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(train_ix_mem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_ix = [yr for yr in ix_score_latest_completed if datasheet.iloc[yr,0] in train_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response word count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem_n = pd.Series([word_count(r) for r in datasheet.iloc[memory_ix,2]],index=memory_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_mem_length = mean(mem_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_mem_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_x_proxy_length = pd.Series([mean_mem_length for t in tech_x_ix],index=tech_x_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tech_x_proxy_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "mem_n = mem_n.append(tech_x_proxy_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#tech ix contains all respondent indices\n",
    "len(mem_n) == len(tech_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_n = pd.Series([word_count(tr) for tr in datasheet.iloc[tech_ix,3]],index=tech_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response sentence count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mem_sentence = pd.Series([sentence_count(mr) for mr in datasheet.iloc[memory_ix,2]],index=memory_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "mean_mem_sentence = mean(mem_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "tech_x_proxy_sentence = pd.Series([mean_mem_sentence for t in tech_x_ix],index=tech_x_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "mem_sentence = mem_sentence.append(tech_x_proxy_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "len(mem_sentence) == len(tech_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_sentence = pd.Series([sentence_count(tr) for tr in datasheet.iloc[tech_ix,3]],index=tech_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#And here you have it\n",
    "#Merged response lengths\n",
    "#sentences_count = pd.Series([mem_sentence[i]+tech_sentence[i] for i in tech_ix],index=tech_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sentences_count.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Richness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem_v = pd.Series([uniq_words(mr) for mr in datasheet.iloc[memory_ix,2]],index=memory_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_v = pd.Series([uniq_words(tr) for tr in datasheet.iloc[tech_ix,3]],index=tech_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem_v1 = pd.Series([single_appearances(mr) for mr in datasheet.iloc[memory_ix,2]],index=memory_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_v1 = pd.Series([single_appearances(tr) for tr in datasheet.iloc[tech_ix,3]],index=tech_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MATTR (window size 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[w for w in window(list_tokenize(datasheet.iloc[496,2]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_tokenize(datasheet.iloc[496,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mem_mattr = []\n",
    "# for ix,mr in zip(memory_ix,datasheet.iloc[memory_ix,2]):\n",
    "#     stripped_mr = list_tokenize(mr)\n",
    "#     if len(stripped_mr) < 10: window_size = len(stripped_mr)\n",
    "#     else: window_size = 10\n",
    "        \n",
    "#     try:\n",
    "#         mem_mattr.append(mean([len(np.unique(mw))/window_size for mw in window(list_tokenize(mr),window_size)]))\n",
    "#     except statistics.StatisticsError:\n",
    "#         print('Stat error at ix',ix, mr)\n",
    "# mem_mattr_a = pd.Series(mem_mattr,index=memory_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ratio uniq words in each 10-word window\n",
    "#avg for all ratios\n",
    "mem_mattr = pd.Series([mean([len(np.unique(mw))/min(len(list_tokenize(mr)),10) for mw in window(list_tokenize(mr),n=min(len(list_tokenize(mr)),10))]) for mr in datasheet.iloc[memory_ix,2]],index=memory_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# mem_mattr_a.equals(mem_mattr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_mattr = pd.Series([mean([len(np.unique(tw))/min(len(list_tokenize(tr)),10) for tw in window(list_tokenize(tr),n=min(len(list_tokenize(tr)),10))]) for tr in datasheet.iloc[tech_ix,3]],index=tech_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem_rate_nn = pd.Series([pos_rate(mr,nn=True) for mr in datasheet.iloc[memory_ix,2]],index=memory_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_rate_nn = pd.Series([pos_rate(tr,nn=True) for tr in datasheet.iloc[tech_ix,3]],index=tech_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem_rate_vb = pd.Series([pos_rate(mr,vb=True) for mr in datasheet.iloc[memory_ix,2]],index=memory_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_rate_vb = pd.Series([pos_rate(tr,vb=True) for tr in datasheet.iloc[tech_ix,3]],index=tech_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pronoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem_rate_pn = pd.Series([pos_rate(mr,pn=True) for mr in datasheet.iloc[memory_ix,2]],index=memory_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_rate_pn = pd.Series([pos_rate(tr,pn=True) for tr in datasheet.iloc[tech_ix,3]],index=tech_ix) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiating with Coordinating Conjunction\n",
    "Counts, 0/1, rate-per-sentences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def coord_conjunctions_init(response):\n",
    "    return sum([1 for sentence in ie_preprocess(response) if sentence[0][1] == 'CC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem_cc_init = pd.Series([catch(lambda:coord_conjunctions_init(mr)) for mr in datasheet.iloc[memory_ix,2]],index=memory_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_cc_init = pd.Series([catch(lambda:coord_conjunctions_init(tr)) for tr in datasheet.iloc[tech_ix,3]],index=tech_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misspellings, words not found in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def misspelt(response):\n",
    "    return sum([1 for word in list_tokenize(response) if word.lower() not in word_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem_misspell = pd.Series([catch(lambda:misspelt(mr)) for mr in datasheet.iloc[memory_ix,2]],index=memory_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_misspell = pd.Series([catch(lambda:misspelt(tr)) for tr in datasheet.iloc[tech_ix,3]],index=tech_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Height of constituency tree, Stanford CoreNLP + nltk Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.khalidalnajjar.com/setup-use-stanford-corenlp-server-python/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "A sample code usage of the python package stanfordcorenlp to access a Stanford CoreNLP server.\n",
    "Written as part of the blog post: https://www.khalidalnajjar.com/how-to-setup-and-use-stanford-corenlp-server-with-python/ \n",
    "\n",
    "cd stanford-corenlp-full-2018-10-05/ \n",
    "java -mx4g -cp \"*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -annotators \"tokenize,ssplit,pos,lemma,parse,sentiment\" -port 9000 -timeout 30000\n",
    "'''\n",
    "\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "import logging\n",
    "import json\n",
    "\n",
    "class StanfordNLP:\n",
    "    def __init__(self, host='http://localhost', port=9000):\n",
    "        self.nlp = StanfordCoreNLP(host, port=port,\n",
    "                                   timeout=30000)  # , quiet=False, logging_level=logging.DEBUG)\n",
    "        self.props = {\n",
    "            'annotators': 'tokenize,ssplit,pos,lemma,ner,parse,depparse,dcoref,relation',\n",
    "            'pipelineLanguage': 'en',\n",
    "            'outputFormat': 'json'\n",
    "        }\n",
    "\n",
    "    def word_tokenize(self, sentence):\n",
    "        return self.nlp.word_tokenize(sentence)\n",
    "\n",
    "    def pos(self, sentence):\n",
    "        return self.nlp.pos_tag(sentence)\n",
    "\n",
    "    def ner(self, sentence):\n",
    "        return self.nlp.ner(sentence)\n",
    "\n",
    "    def parse(self, sentence):\n",
    "        return self.nlp.parse(sentence)\n",
    "\n",
    "    def dependency_parse(self, sentence):\n",
    "        return self.nlp.dependency_parse(sentence)\n",
    "\n",
    "    def annotate(self, sentence):\n",
    "        return json.loads(self.nlp.annotate(sentence, properties=self.props))\n",
    "\n",
    "    @staticmethod\n",
    "    def tokens_to_dict(_tokens):\n",
    "        tokens = defaultdict(dict)\n",
    "        for token in _tokens:\n",
    "            tokens[int(token['index'])] = {\n",
    "                'word': token['word'],\n",
    "                'lemma': token['lemma'],\n",
    "                'pos': token['pos'],\n",
    "                'ner': token['ner']\n",
    "            }\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tree import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sNLP = StanfordNLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem_syntree = pd.Series([Tree.fromstring(str(sNLP.parse(mr))).height() for mr in datasheet.iloc[memory_ix,2]],index=memory_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_syntree = pd.Series([Tree.fromstring(str(sNLP.parse(tr))).height() for tr in datasheet.iloc[tech_ix,3]],index=tech_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentence complexity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scipy.stats.pearsonr(mem_syntree,tech_syntree[memory_ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scipy.stats.skew(mem_syntree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scipy.stats.skew(tech_syntree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scipy.stats.kurtosis(mem_syntree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scipy.stats.kurtosis(tech_syntree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scipy.stats.spearmanr(mem_syntree,tech_syntree[memory_ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Lynten/stanford-corenlp/issues/59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduced sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reduced sentences:\n",
    "#  Following the definition set out by [25], this feature represents those subordinated sentences without a conjunction but with nominal verb forms (which are either participles or gerund). To obtain the count for this feature, the frequencies of POS tags (VBG and VBN) are used.\n",
    "\n",
    "# From <https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5237556/#CR25> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem_reduced = pd.Series([sum([1 for (word,tag) in sNLP.pos(mr) if tag in ['VBG','VBN']]) for mr in datasheet.iloc[memory_ix,2]],index=memory_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_reduced = pd.Series([sum([1 for (word,tag) in sNLP.pos(tr) if tag in ['VBG','VBN']]) for tr in datasheet.iloc[tech_ix,3]],index=tech_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scipy.stats.spearmanr(mem_reduced,tech_reduced[memory_ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis/Emotion recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for prompt in datasheet.iloc[memory_ix,2]:\n",
    "    print(prompt)\n",
    "    print(vader.polarity_scores(prompt))\n",
    "    print('--------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One of my fondest childhood memories is how much I loved growing up around so much family.  My father's family all lived near us and we together frequently.  It was always so much happiness when everyone was together.\n",
    "# {'neg': 0.0, 'neu': 0.782, 'pos': 0.218, 'compound': 0.8583}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I have a very clear memeory of last time I saw my father before he passed away in an accident. I was 14 years old.   I do not need to explain why this is important for me .\n",
    "# {'neg': 0.084, 'neu': 0.788, 'pos': 0.128, 'compound': 0.1513}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem_sent_neg = pd.Series([vader.polarity_scores(prompt)['neg'] for prompt in datasheet.iloc[memory_ix,2]],index=memory_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem_sent_neu = pd.Series([vader.polarity_scores(prompt)['neu'] for prompt in datasheet.iloc[memory_ix,2]],index=memory_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem_sent_pos = pd.Series([vader.polarity_scores(prompt)['pos'] for prompt in datasheet.iloc[memory_ix,2]],index=memory_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem_sent_cpd = pd.Series([vader.polarity_scores(prompt)['compound'] for prompt in datasheet.iloc[memory_ix,2]],index=memory_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_sent_neg = pd.Series([vader.polarity_scores(prompt)['neg'] for prompt in datasheet.iloc[tech_ix,3]],index=tech_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_sent_neu = pd.Series([vader.polarity_scores(prompt)['neu'] for prompt in datasheet.iloc[tech_ix,3]],index=tech_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_sent_pos = pd.Series([vader.polarity_scores(prompt)['pos'] for prompt in datasheet.iloc[tech_ix,3]],index=tech_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_sent_cpd = pd.Series([vader.polarity_scores(prompt)['compound'] for prompt in datasheet.iloc[tech_ix,3]],index=tech_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for blob in datasheet.iloc[memory_ix,2]:\n",
    "    print(blob)\n",
    "    print(TextBlob(blob).sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem_sent_pol = pd.Series([TextBlob(datasheet.iloc[mr,2]).sentiment[0] for mr in memory_ix],index=memory_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem_sent_subj = pd.Series([TextBlob(datasheet.iloc[mr,2]).sentiment[1] for mr in memory_ix],index=memory_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_sent_pol = pd.Series([TextBlob(datasheet.iloc[tr,3]).sentiment[0] for tr in tech_ix],index=tech_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_sent_subj = pd.Series([TextBlob(datasheet.iloc[tr,3]).sentiment[1] for tr in tech_ix],index=tech_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical density - rate (nouns+vb+adj+adv)/n : (gets at semantic anal?)\n",
    "file:///Users/renee/Downloads/2273-Article%20Text-4927-1-10-20100218.pdf\n",
    "Lexical density is the term most often used for describing the proportion of\n",
    "content words (nouns, verbs, adjectives, and often also adverbs) to the total\n",
    "number of words. By investigating this, we receive a notion of information\n",
    "packaging; a text with a high proportion of content words contains more\n",
    "information than a text with a high proportion of function words (prepositions,\n",
    "interjections, pronouns, conjunctions and count words). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea density "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/andrecunha/idd3/blob/master/run.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEMANTIC (high rate of function words? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People + object reference words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem_rate_cloc = pd.Series([pos_rate(mr,cloc=True) for mr in datasheet.iloc[memory_ix,2]],index=memory_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_rate_cloc = pd.Series([pos_rate(tr,cloc=True) for tr in datasheet.iloc[tech_ix,3]],index=tech_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem_rate_func = pd.Series([func_rate(mr) for mr in datasheet.iloc[memory_ix,2]],index=memory_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_rate_func = pd.Series([func_rate(tr) for tr in datasheet.iloc[tech_ix,3]],index=tech_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem_feature_list = [mem_n,mem_sentence,mem_v,mem_v1,mem_mattr,mem_rate_nn,mem_rate_vb,mem_rate_pn,mem_rate_cloc,mem_cc_init,mem_misspell,mem_syntree,mem_reduced,mem_sent_neg,mem_sent_neu,mem_sent_pos,mem_sent_cpd,mem_sent_pol,mem_sent_subj,mem_rate_func,mem_honore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem_feature_df = pd.DataFrame(mem_feature_list).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_feature_list = [tech_n,tech_sentence,tech_v,tech_v1,tech_mattr,tech_rate_nn,tech_rate_vb,tech_rate_pn,tech_rate_cloc,tech_cc_init,tech_misspell,tech_syntree,tech_reduced,tech_sent_neg,tech_sent_neu,tech_sent_pos,tech_sent_cpd,tech_sent_pol,tech_sent_subj,tech_rate_func,tech_honore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_feature_df = pd.DataFrame(tech_feature_list).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = mem_feature_df.join(tech_feature_df,how='outer',lsuffix='_mem_features',rsuffix='_tech_features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace nan with col means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X.fillna(X.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear: Pearsons, Spearmans - helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pearsons_corr(y,X=X):\n",
    "    pearsons_sig = []\n",
    "    for col_name in X.columns:\n",
    "        p_corr = scipy.stats.pearsonr(X[col_name],y)\n",
    "        print('pearsons:',col_name,p_corr)\n",
    "        if p_corr[1] < 0.05:\n",
    "            pearsons_sig += [(col_name, p_corr)]\n",
    "    return pearsons_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def spearmans_corr(y,X=X):\n",
    "    spearmans_sig = []\n",
    "    for col_name in X.columns:\n",
    "        s_corr = scipy.stats.spearmanr(X[col_name],y)\n",
    "        print('spearmans:',col_name,s_corr)\n",
    "        if s_corr[1] < 0.05:\n",
    "            spearmans_sig += [(col_name,s_corr)]\n",
    "    return spearmans_sig\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chrisalbon.com/machine_learning/linear_regression/adding_interaction_terms/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonferonni Adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Correlation - Pearson's R [Linear Relationship] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.columns = ['mem_n','mem_sentence','mem_v','mem_v1','mem_mattr','mem_rate_nn','mem_rate_vb','mem_rate_pn','mem_rate_cloc','mem_cc_init','mem_misspell','mem_syntree','mem_reduced','mem_sent_neg','mem_sent_neu','mem_sent_pos','mem_sent_cpd','mem_sent_pol','mem_sent_subj','mem_rate_func','mem_honore','tech_n','tech_sentence','tech_v','tech_v1','tech_mattr','tech_rate_nn','tech_rate_vb','tech_rate_pn','tech_rate_cloc','tech_cc_init','tech_misspell','tech_syntree','tech_reduced','tech_sent_neg','tech_sent_neu','tech_sent_pos','tech_sent_cpd','tech_sent_pol','tech_sent_subj','tech_rate_func','tech_honore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col_name in X.columns:\n",
    "    print(col_name,scipy.stats.skew(X[col_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col_name in X.columns:\n",
    "    print(col_name,scipy.stats.kurtosis(X[col_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target: Diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## y: clinical diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lancet_target = [0 if record_id=='NL' else 1 for record_id in lancet_id_ix[17]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lancet_target = pd.Series(lancet_target,index=lancet_id_ix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#order target by X.index\n",
    "#ix_id_series indexed by X.index\n",
    "#lancet_target.loc indexed by ix_id_series record_id\n",
    "y_diagnosis = [lancet_target.loc[ix_id_series[ix]] for ix in X.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SCI considered cognitive typical, 0\n",
    "lancet_target_sci_ct = [0 if record_id=='NL' or record_id =='SCI' else 1 for record_id in lancet_id_ix[17]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lancet_target_sci_ct = pd.Series(lancet_target_sci_ct,index=lancet_id_ix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_sci_ct = pd.Series([lancet_target_sci_ct.loc[ix_id_series[ix]] for ix in X.index],index=X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_sig_diagnosis = pearsons_corr(y_sci_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_sig_diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_sig_diagnosis = spearmans_corr(y_sci_ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_sig_diagnosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y: MMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_mmse = pd.Series([y for y in datasheet.iloc[ix_score_latest_completed,74]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_mmse = y_mmse.fillna(math.floor(y_mmse.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_sig_mmse = pearsons_corr(y_mmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_sig_mmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_sig_mmse = spearmans_corr(y_mmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_sig_mmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Targets: Language + Verbal Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y: Picture vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pic_vocab = datasheet.iloc[ix_score_latest_completed,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pic_vocab.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_sig_picvocab = spearmans_corr(y_pic_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_sig_picvocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_sig_picvocab = pearsons_corr(y_pic_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_sig_picvocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### y: Oral reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_oral_reading = datasheet.iloc[ix_score_latest_completed,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_sig_orreading = pearsons_corr(y_oral_reading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_sig_orreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_sig_orreading = spearmans_corr(y_oral_reading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_sig_orreading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_oral_reading.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem_rate_cloc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_oral_reading.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(y_oral_reading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min(y_oral_reading)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
