{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as DT\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "import math\n",
    "from __future__ import division\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "import string\n",
    "import pprint\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import vaderSentiment.vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# vaderSentiment.vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. AlzU Survey Data Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ALZU_RESPONSES_FILE_PATH = '../Data_Folder/2018.09.18_AlzU_ANU-DRI + Responses_Numerical.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine first two rows to generate column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "two_header_rows = pd.read_csv(ALZU_RESPONSES_FILE_PATH, header=None, nrows = 2, delimiter = \",\", keep_default_na=False)\n",
    "\n",
    "transformed_header_row = []\n",
    "\n",
    "most_recent_col_first_row = None\n",
    "for column in two_header_rows:\n",
    "    primary_header = two_header_rows[column][0]\n",
    "    sub_header = two_header_rows[column][1]\n",
    "#     print(primary_header + \": \" + sub_header)\n",
    "    if(primary_header != \"\"):\n",
    "        most_recent_col_first_row = primary_header\n",
    "    transformed_header_row.append(most_recent_col_first_row + \"_\" + sub_header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data with column names\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alzu_survey_data = pd.read_csv(ALZU_RESPONSES_FILE_PATH, header=None, skiprows = [0,1], delimiter = \",\", keep_default_na=False, names=transformed_header_row)\n",
    "\n",
    "# alzu_survey_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alzu_survey_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx, col in enumerate(alzu_survey_data.columns):\n",
    "    print(str(idx) + \" : \" + str(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tech_prompt_idx = 123\n",
    "tec_prompt_idx = 124\n",
    "\n",
    "tech_prompt = alzu_survey_data.columns[tech_prompt_idx]\n",
    "tec_prompt = alzu_survey_data.columns[tec_prompt_idx]\n",
    "\n",
    "print(\"Num respondents to memory prompt: \"+ str(alzu_survey_data[alzu_survey_data[tech_prompt] != ''].shape[0]))\n",
    "print(\"Num respondents to tech prompt: \"+ str(alzu_survey_data[alzu_survey_data[tec_prompt] != ''].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I am exploring how many users have submitted multiple surveys.  After inspecting many of these users responses, I decide to only keep their most recent survey response, because empirically it seems like that one has the richest data -- the other submissions tend to be minutes before and have no responses, making me think they were having technical difficulties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_counts = alzu_survey_data['uid_Open-Ended Response'].value_counts()\n",
    "\n",
    "num_users_multiple_submissions = val_counts[val_counts != 1].shape[0]\n",
    "\n",
    "print(\"There are \" + str(num_users_multiple_submissions) + \" users who submitted multiple surveys.\")\n",
    "# print(val_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alzu_survey_data.drop_duplicates('uid_Open-Ended Response', keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. CFT Data Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CFT_FILE_PATH = '../Data_Folder/2018.09_CFT_all.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cft_data = pd.read_csv(CFT_FILE_PATH, delimiter = \",\", keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cft_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cft_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploration of Dr. Isaacson's Questions about how many times people have taken the CFT for those that have registered between August 22nd and March 7th.\n",
    "\n",
    "- Do we use \"RegDate\" in the CFT dataset as the reference date, or do we have to look into the User list for that?\n",
    "- Do we care about those that registered or do we need a Final Score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cft_data['RegDate'] = pd.to_datetime(cft_data['RegDate'])\n",
    "\n",
    "from datetime import datetime\n",
    "start_string = 'Aug 22 2017'\n",
    "end_string = 'Mar 05 2018'\n",
    "start_date = datetime.strptime(start_string, '%b %d %Y')\n",
    "end_date = datetime.strptime(end_string, '%b %d %Y')\n",
    "mask = (cft_data['RegDate'] > start_date) & (cft_data['RegDate'] <= end_date)\n",
    "# Select the sub-DataFrame:\n",
    "\n",
    "cft_btwn_dates = cft_data.loc[mask]\n",
    "\n",
    "print (str(cft_btwn_dates.shape[0]) + \" users have registered for the CFT between \" + start_string + \" and \" + end_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cft_btwn_dates.groupby('EmailAddress').count().groupby('UserID').count().ix[:, 2]\n",
    "# for idx, row in cft_data.groupby('EmailAddress').count().groupby('UserID').count():\n",
    "#     print idx, row['FirstName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cft_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CFT Summary Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_cft = cft_data.drop_duplicates('EmailAddress', keep='first') # takes the first (oldest) score\n",
    "\n",
    "print(\"Number of tests taken: \" + str(cft_data.shape[0]))\n",
    "print(\"Number of unique users: \" + str(unique_cft.shape[0]))\n",
    "\n",
    "unique_cft.groupby('Gender').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for cft_col in unique_cft.columns:\n",
    "    print(cft_col)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Email-UserID Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALZU_USERS_FILE_PATH = '../Data_Folder/2018.09_alzu_users.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_data = pd.read_csv(ALZU_USERS_FILE_PATH, delimiter = \",\", keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# user_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations + Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_data['Joined'] = pd.to_datetime(user_data['Joined'])\n",
    "print(user_data.shape)\n",
    "user_data = user_data.drop_duplicates('Email', keep='last')\n",
    "print(user_data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Summary Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Number of total users: \" + str(user_data.shape[0]))\n",
    "gender_grouping = user_data['Gender'].value_counts()\n",
    "num_females = gender_grouping['F']\n",
    "num_males = gender_grouping['M']\n",
    "\n",
    "print(\"Number of male users: \" + str(num_males))\n",
    "print(\"Number of female users: \" + str(num_females))\n",
    "print(\"Male:Female Ratio ≈ 1:\" + str(num_females / num_males))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "date_indexed = user_data.set_index(user_data[\"Joined\"])\n",
    "date_indexed['count'] = 1\n",
    "counts = date_indexed['count'].resample('M').sum().dropna()\n",
    "\n",
    "counts = counts[2:] #get rid of 2 obviously incorrect ones dating way back\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "cumulative_counts = counts.cumsum()\n",
    "cumulative_counts.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JOIN DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_cft.rename(index=str, columns={\"EmailAddress\": \"Email\"}, inplace=True)\n",
    "alzu_survey_data.rename(index=str, columns={\"uid_Open-Ended Response\": \"User ID\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "users_joined_cognitive = user_data.merge(unique_cft, on=\"Email\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(users_joined_cognitive.shape)\n",
    "# users_joined_cognitive.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data = users_joined_cognitive.merge(alzu_survey_data, on=\"User ID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Exploration of Combined Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = all_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for idx, col in enumerate(cols):\n",
    "    print(str(idx) + \" : \" + str(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "memory_prompt_idx = 176\n",
    "tech_prompt_idx = 177\n",
    "\n",
    "memory_prompt = cols[memory_prompt_idx]\n",
    "tech_prompt = cols[tech_prompt_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Peform some General Data Transformations that do not feel model or experiment-specific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop all rows in which they have neither Survey Data or CFT data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "survey_and_cft_columns = cols[range(9, 180)]\n",
    "\n",
    "all_data.dropna(how='all', subset=[survey_and_cft_columns], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename gender column and replace with 0 (female) and 1 (male)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data.rename(index=str, columns={\"Gender_x\": \"Gender\"}, inplace=True)\n",
    "\n",
    "all_data['Gender'].replace(['F','M'],[0,1],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an Age column based on the DoB column and get some stats on age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "now = pd.Timestamp(DT.datetime.now())\n",
    "\n",
    "\n",
    "dobs = pd.to_datetime(all_data[all_data['DoB'].notnull()]['DoB'])\n",
    "dobs = dobs.where(dobs < now, dobs -  np.timedelta64(100, 'Y'))\n",
    "ages = (now - dobs).astype('<m8[Y]')    # 3\n",
    "\n",
    "all_data['Age'] = ages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "join_dates = pd.to_datetime(all_data[all_data['Joined'].notnull()]['Joined'])\n",
    "\n",
    "join_dates = join_dates.where(join_dates < now, join_dates -  np.timedelta64(100, 'Y'))\n",
    "\n",
    "days_as_member = (now - join_dates).astype('<m8[D]')\n",
    "\n",
    "all_data['Days_As_Member'] = days_as_member"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace NANs in the prompt columns with empty strings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data[memory_prompt].fillna('', inplace=True)\n",
    "all_data[tech_prompt].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format Scores as Numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data['FinalScore'] = pd.to_numeric(all_data['FinalScore'])\n",
    "all_data['MouseScore'] = pd.to_numeric(all_data['FinalScore'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Stats on Transformed \"All Data\" Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(all_data.shape)\n",
    "print(\"Average Age:\")\n",
    "print(all_data['Age'].mean())\n",
    "print(\"Median Age:\")\n",
    "print(all_data['Age'].median())\n",
    "\n",
    "gender_grouping = all_data['Gender'].value_counts()\n",
    "num_females = gender_grouping[0]\n",
    "num_males = gender_grouping[1]\n",
    "\n",
    "print(\"Number of male users: \" + str(num_males))\n",
    "print(\"Number of female users: \" + str(num_females))\n",
    "print(\"Male:Female Ratio ≈ 1:\" + str(num_females / num_males))\n",
    "\n",
    "print(\"Average membership Duration:\")\n",
    "print(all_data['Days_As_Member'].mean())\n",
    "\n",
    "print(all_data[((all_data[memory_prompt] != \"\") | (all_data[tech_prompt] != \"\"))].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for users with cognitive scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1:  Ignore the written portion and identify best ML model for those with Cognitive Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Summary Stats on those that have taken Cognitive Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cog_takers = all_data[all_data['FinalScore'].notnull()]\n",
    "print(\"Num respondents with final cognitive score: \" + str(cog_takers.shape[0]))\n",
    "\n",
    "print(\"Average CFT Score: \" + str(cog_takers['FinalScore'].mean()))\n",
    "print(\"Median CFT Score: \" + str(cog_takers['FinalScore'].median()))\n",
    "print(\"Standard Deviation of CFT Score: \" + str(cog_takers['FinalScore'].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "red_zone = cog_takers[cog_takers['FinalScore'] <= 38]\n",
    "orange_zone = cog_takers[((cog_takers['FinalScore'] > 38) & (cog_takers['FinalScore'] <= 43))]\n",
    "green_zone = cog_takers[cog_takers['FinalScore'] > 43]\n",
    "\n",
    "objects = ('Significant Risk (<38)', 'At Risk (<43, >38)', 'Normal (>43)')\n",
    "y_pos = np.arange(len(objects))\n",
    "performance = [red_zone.shape[0],orange_zone.shape[0], green_zone.shape[0]]\n",
    " \n",
    "barlist=plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "\n",
    "barlist[0].set_color('#FF0000')\n",
    "barlist[1].set_color('#FFBF00')\n",
    "barlist[2].set_color('g')\n",
    "\n",
    "plt.xticks(y_pos, objects)\n",
    "plt.ylabel('Number of Participants')\n",
    "plt.title('AlzU CFT Performance')\n",
    " \n",
    "plt.show()\n",
    "\n",
    "print(performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2: Pre-Process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = all_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For those that have not taken CFT, replace value with mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_cft_score = cog_takers['FinalScore'].mean()\n",
    "\n",
    "X['FinalScore'].fillna(avg_cft_score, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-process the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Convert the alzU columns to ints, and Replace \"NaN\" and empty strings with the mean of column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx_of_first_alzu_q = 62\n",
    "idx_of_last_non_prompt_alz_u_q = 175\n",
    "open_ended_alzu_indices = [64, 99, 120, 122, 124, 134, 171,175] #columns where they could enter in anything \n",
    "\n",
    "for col_idx in range(idx_of_first_alzu_q, idx_of_last_non_prompt_alz_u_q + 1):\n",
    "    col_name = cols[col_idx]\n",
    "    print(col_name)\n",
    "    if (col_idx in open_ended_alzu_indices):\n",
    "        continue\n",
    "    print(X[col_name].unique())\n",
    "    \n",
    "    # first convert all nan and empty to -1, so that the conversion of the column to astype int doesn't break\n",
    "    X[col_name].fillna(-1, inplace=True)\n",
    "    print(X[col_name].unique())\n",
    "    X[col_name].replace([''], [-1], inplace=True)\n",
    "    X[col_name].replace(['1.11111E+77'], [-1], inplace=True) # handle the bizarre entry that was breaking this\n",
    "    X[col_name] = (X[col_name]).astype(int)\n",
    "    \n",
    "    X[col_name].replace([-1], [np.NaN], inplace=True)\n",
    "    column_mean = X[col_name].mean()\n",
    "    X[col_name].fillna(column_mean, inplace=True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(str(col_idx) + \": \" + col_name + \": \" +str(X[col_name].dtype))\n",
    "    print(X[col_name].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform some true false columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_true_false_col(col_name):\n",
    "    replaced_NaNs = X[col_name].fillna(-1)\n",
    "    replaced_t_f = replaced_NaNs.replace([False,True],[0,1])    \n",
    "    replaced_t_f = replaced_t_f.astype(int)\n",
    "    replaced_t_f.replace([-1], [np.NaN], inplace=True)\n",
    "    column_mean = replaced_t_f.mean()\n",
    "    replaced_t_f.fillna(column_mean, inplace=True)\n",
    "    return replaced_t_f\n",
    "\n",
    "\n",
    "X['MemConcern'] = transform_true_false_col('MemConcern')\n",
    "X['ForgetFriends'] = transform_true_false_col('ForgetFriends')\n",
    "X['PutThings'] = transform_true_false_col('PutThings')\n",
    "X['ForgetWords'] = transform_true_false_col('ForgetWords')\n",
    "X['LoseWay'] = transform_true_false_col('LoseWay')\n",
    "X['IsMemoryWorse'] = transform_true_false_col('IsMemoryWorse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform some yes no and other string columns...Taking some liberties here with how to encode them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X['FamilyHistory'].fillna(-1, inplace=True)\n",
    "X['FamilyHistory'].replace(['', 'no', 'yes'], [-1,0,1], inplace=True)\n",
    "X['FamilyHistory'].replace([-1], [np.NaN], inplace=True)\n",
    "\n",
    "X['FamilyHistoryAge'].fillna(-1, inplace=True)\n",
    "X['FamilyHistoryAge'].replace(['', 'Select', 'none', 'mild', 'other', 'alzh'], [-1,-1,0,1,2,3], inplace=True)\n",
    "X['FamilyHistoryAge'].replace([-1], [np.NaN], inplace=True)\n",
    "\n",
    "X['Supplements'].fillna(-1, inplace=True)\n",
    "X['Supplements'].replace(['Select', 'No', 'Yes'], [-1,0,1], inplace=True)\n",
    "X['Supplements'].replace([-1], [np.NaN], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns that we know are not good for ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = X.drop(['Joined', 'User ID', 'First Name', 'Email', 'Lesson Group', 'UserID', 'FirstName', 'Respondent ID_', 'Collector ID_', 'IP Address_', 'Email Address_', 'First Name', 'Last Name_', 'Gender_y', 'DoB', 'joined_Open-Ended Response', 'email_Open-Ended Response', 'Start Date_', 'End Date_', 'First Name_', 'Custom Data 1_'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop columns from the CFT Dataset, THAT MAY BE OF USE LATER, but are just of inconvenient data types...Also we need to confirm that they do not influence the FinalScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = X.drop(['RegDate','DateTaken','Homoscyteine', 'Source', 'Working', 'CompUser', 'DeviceUsed'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = X.drop(['recallscore', 'placescore', 'MouseScore', 'Country', 'Postcode', 'Ethnicy', 'Interrupted', 'Functioned', 'FastInternet', 'ScreenClear', 'MaritalStatus', 'Dependents', 'Occupation', 'PrimaryIncome', 'HouseholdIncome', 'PrimaryIncomeOccupation', 'FirstPriority', 'SecondPriority', 'OtherPriority', 'ass', 'combocs'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many questions, particularly in the CFT portion, where there are no responses.  We drop those columns below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for col in X.columns:\n",
    "    if (X[col].dtype != 'int64' and X[col].dtype != 'float64'):\n",
    "        if(len(X[col].unique()) == 1 and X[col].unique() == ['']):\n",
    "            X.drop(col, axis=1, inplace=True)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the alzU columns where the format is open-ended.  Some of them have interesting data, but hard to think about how to convert them into numeric at this time.  Come back to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for open_ended_idx in open_ended_alzu_indices:\n",
    "    col_name = cols[open_ended_idx]\n",
    "    X.drop(col_name, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the written responses field for this round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.drop(memory_prompt, axis=1, inplace=True)\n",
    "X.drop(tech_prompt, axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make sure that all columns are ints or floats and convert NaN into mean for all columns\n",
    "for col in X.columns:\n",
    "    if (X[col].dtype !='int64' and X[col].dtype !='float64'):\n",
    "        print(\"PROBLEM! COLUMN IS NOT IN NUMERICAL FORMAT\")\n",
    "        print(col + \": \" + str(X[col].dtype))\n",
    "    else:\n",
    "        column_mean = X[col].mean()\n",
    "        X[col].fillna(column_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "def run_and_print_models(x_train, y_train, x_test, y_test):\n",
    "    features_importance_dict = {}\n",
    "    \n",
    "    #increments a features \"importance\" based on its weight in a model and the performance of that model\n",
    "    def update_features_dict(feature_weights, performance):\n",
    "#         print(\"FEATURE WEIGHTS\")\n",
    "#         print(feature_weights)       \n",
    "        absolute_feature_weights = np.absolute(feature_weights)\n",
    "#         print(\"ABS VALUE\")\n",
    "#         print(absolute_feature_weights)\n",
    "        sorted_feature_weights_indices = np.argsort(absolute_feature_weights) # list of feature indices, sorted ascending by absolute value of feature weight \n",
    "#         print(\"Num FEATURES: \" + str(len(feature_weights)))\n",
    "        for ranking, feature_index in enumerate(sorted_feature_weights_indices):            \n",
    "            feature_name = x_train.columns[feature_index]\n",
    "#             print(\"Feature: \")\n",
    "#             print(feature_name)\n",
    "#             print(\"Ranking: \")\n",
    "#             print(ranking)\n",
    "            feature_ranking = ranking * performance\n",
    "#             print(\"Travis Feature Ranking Rating:\")\n",
    "#             print(feature_ranking)\n",
    "#             print(feature_ranking)\n",
    "            if feature_name in features_importance_dict:\n",
    "                features_importance_dict[feature_name] += feature_ranking\n",
    "            else:\n",
    "                features_importance_dict[feature_name] = feature_ranking\n",
    "    \n",
    "    def get_top_n_features(n, feature_weights):\n",
    "        top_n_idx = np.argsort(feature_weights)[-n:] #SHOULD I BE USING ABSOLUTE VALUE?\n",
    "        top_n_features = [x_train.columns[i] for i in top_n_idx]\n",
    "        return top_n_features\n",
    "    \n",
    "    models = {\n",
    "        \"3-Nearest-Neighbors\": KNeighborsClassifier(n_neighbors=3),\n",
    "        \"Linear Regression\": LinearRegression(),\n",
    "        \"Decision Tree\": tree.DecisionTreeClassifier(),\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "                                 random_state=0),\n",
    "        \"Support Vector Machines\": svm.SVC(kernel='linear')    \n",
    "    }\n",
    "    \n",
    "    for name, model in models.iteritems():\n",
    "        model.fit(x_train, y_train)\n",
    "        performance = model.score(x_test, y_test)\n",
    "        print(name + \" Classification Accuracy: \" + str(performance))\n",
    "        if (name == \"3-Nearest-Neighbors\"):\n",
    "            continue\n",
    "        elif (name == \"Support Vector Machines\"):\n",
    "            feature_weights = model.coef_[0]\n",
    "        elif(name==\"Linear Regression\"):\n",
    "            feature_weights = model.coef_\n",
    "        else:\n",
    "            feature_weights = model.feature_importances_\n",
    "#         print(name + \" Top Features: \")\n",
    "#         for idx, feat in enumerate(get_top_n_features(5, feature_weights)):\n",
    "#             print(str(idx + 1) + \": \" + feat)\n",
    "        \n",
    "        update_features_dict(feature_weights, performance)\n",
    "    import operator\n",
    "\n",
    "    sorted_x = sorted(features_importance_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    for idx, feat_score_and_weight in enumerate(sorted_x):\n",
    "        if (idx > 9):\n",
    "            break\n",
    "        print(str(idx + 1) + \": \" + str(feat_score_and_weight))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1: Two-Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify cog scores on Median (above median = 1, below median = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# X_two_class = X.copy()\n",
    "# median_score = X_two_class['FinalScore'].median()\n",
    "\n",
    "# X_two_class.loc[X_two_class['FinalScore'] < median_score, 'FinalScore'] = 0\n",
    "# X_two_class.loc[X_two_class['FinalScore'] >= median_score, 'FinalScore'] = 1\n",
    "\n",
    "# y_two_class = X_two_class['FinalScore']\n",
    "# X_two_class = X_two_class.drop(['FinalScore'], axis=1)\n",
    "\n",
    "# X_train_two, X_test_two, y_train_two, y_test_two = train_test_split(X_two_class,y_two_class)\n",
    "\n",
    "\n",
    "# print(X_train_two.shape)\n",
    "# print(X_test_two.shape)\n",
    "# print(y_train_two.shape)\n",
    "# print(y_test_two.shape)\n",
    "\n",
    "# print(\"Label Division...........\")\n",
    "# print(y_two_class[y_two_class == 0].count())\n",
    "# print(y_two_class[y_two_class == 1].count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run_and_print_models(X_train_two, y_train_two, X_test_two, y_test_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2: Three-Class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split based on the scoring categories provided here: http://www.foodforthebrain.org/alzheimers-prevention/take-the-test/interpreting-results.aspx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_three_class = X.copy()\n",
    "\n",
    "# X_three_class.loc[X_three_class['FinalScore'] <= 38, 'FinalScore'] = 0\n",
    "# X_three_class.loc[((X_three_class['FinalScore'] > 38) & (X_three_class['FinalScore'] <= 43)), 'FinalScore'] = 1\n",
    "# X_three_class.loc[X_three_class['FinalScore'] > 43, 'FinalScore'] = 2\n",
    "\n",
    "# y_three_class = X_three_class['FinalScore']\n",
    "# X_three_class = X_three_class.drop(['FinalScore'], axis=1)\n",
    "\n",
    "# X_train_three, X_test_three, y_train_three, y_test_three = train_test_split(X_three_class,y_three_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run_and_print_models(X_train_three, y_train_three, X_test_three, y_test_three)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2: Analysis of best models for Writing Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# import nltk\n",
    "# import re\n",
    "# from collections import Counter\n",
    "# import string\n",
    "# import pprint\n",
    "\n",
    "# have_writing = all_data[((all_data[memory_prompt] != \"\") | (all_data[tech_prompt] != \"\"))].copy()\n",
    "\n",
    "# print(have_writing.shape)\n",
    "\n",
    "# writing_samples = have_writing.iloc[:,memory_prompt_idx:tech_prompt_idx + 1]\n",
    "\n",
    "# print(writing_samples.shape)\n",
    "\n",
    "# cft_scores = have_writing['FinalScore']\n",
    "\n",
    "# print(cft_scores.shape)\n",
    "\n",
    "# #replace NaN with mean\n",
    "# cft_median_score = cft_scores.median()\n",
    "# cft_mean_score = cft_scores.mean()\n",
    "\n",
    "# cft_scores.fillna(cft_mean_score, inplace=True)\n",
    "# print(cft_median_score)\n",
    "# print(cft_scores.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(cft_scores.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# have_writing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2.1: Two-class Classification using TF-IDF features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.where.html#pandas.Series.where\n",
    "# two_class_cft = cft_scores.copy()\n",
    "# two_class_cft = two_class_cft.where(two_class_cft >= cft_median_score, 0) \n",
    "# two_class_cft = two_class_cft.where(two_class_cft < cft_median_score, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ## Models to test\n",
    "# model_dict = {\n",
    "#     'Naive Bayes': MultinomialNB(),\n",
    "#     'Support Vector Machines': SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42,max_iter=5, tol=None),\n",
    "#     'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "#                                  random_state=0),\n",
    "#     'Decision Tree': tree.DecisionTreeClassifier(),\n",
    "#     \"3-Nearest-Neighbors\": KNeighborsClassifier(n_neighbors=3)\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit Memory Responses Alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mem_prompt = have_writing.iloc[:,memory_prompt_idx]\n",
    "\n",
    "# print(mem_prompt.shape)\n",
    "# mem_prompt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# mem_train_two, mem_test_two, score_train_two, score_test_two = train_test_split(mem_prompt,two_class_cft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# best_mem_clf = {\n",
    "#     'Name': None,\n",
    "#     'FitModel':None,\n",
    "#     'Score': 0\n",
    "# }\n",
    "# for model_name in model_dict:\n",
    "#     model = model_dict[model_name]\n",
    "#     clf = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', model)])\n",
    "#     clf.fit(mem_train_two, score_train_two)\n",
    "\n",
    "#     predicted = clf.predict(mem_test_two)\n",
    "#     predict_accuracy = np.mean(predicted == score_test_two)\n",
    "#     if (predict_accuracy >= best_mem_clf['Score']):\n",
    "#         best_mem_clf['Name'] = model_name\n",
    "#         best_mem_clf['FitModel'] = clf\n",
    "#         best_mem_clf['Score'] = predict_accuracy\n",
    "#     print(model_name + \" Prediction Accuracy:\")\n",
    "#     print(predict_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# best_mem_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for step in best_mem_clf['FitModel'].steps: \n",
    "#     if step[0] == 'clf':\n",
    "#         feature_importances = step[1].feature_importances_ #get_params()\n",
    "\n",
    "# feature_importances.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit Tech Responses Alone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tech_prompt_responses = have_writing.iloc[:,tech_prompt_idx]\n",
    "# print(tech_prompt_responses.shape)\n",
    "# print(tech_prompt_responses.head())\n",
    "\n",
    "# tech_train_two, tech_test_two, score_train_two, score_test_two = train_test_split(tech_prompt_responses,two_class_cft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for model_name in model_dict:\n",
    "#     model = model_dict[model_name]\n",
    "#     clf = Pipeline([('vect', CountVectorizer()),('tfidf', TfidfTransformer()),('clf', model)])\n",
    "#     clf.fit(tech_train_two, score_train_two)\n",
    "\n",
    "#     predicted = clf.predict(tech_test_two)\n",
    "#     predict_accuracy = np.mean(predicted == score_test_two) \n",
    "#     print(model_name + \" Prediction Accuracy:\")\n",
    "#     print(predict_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2.2: Three-class Classification using TF-IDF features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# three_class_cft = cft_scores.copy()\n",
    "\n",
    "# three_class_cft = three_class_cft.where(three_class_cft <= 38, 0)\n",
    "# three_class_cft = three_class_cft.where((three_class_cft > 38 & three_class_cft <=43), 1)\n",
    "# three_class_cft = three_class_cft.where(three_class_cft > 43, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def split_words(sample_string):\n",
    "#     return re.sub('['+string.punctuation.replace('\\'','')+']',' ',sample_string).split()\n",
    "\n",
    "# # Number of Words in Responses\n",
    "# print(len(split_words(sample_mem)))\n",
    "# print(len(split_words(sample_tech)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Number of Unique Words in Response\n",
    "# print(len(np.unique(split_words(sample_mem))))\n",
    "# print(len(np.unique(split_words(sample_tech))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #Honore's lexical richness calculation\n",
    "\n",
    "# def calc_honores(sample_string):\n",
    "#     split_string = re.sub('['+string.punctuation.replace('\\'','')+']',' ',sample_string).split()\n",
    "#     N = len(split_string) #number of words\n",
    "#     uniq_words, counts = np.unique(split_string, return_counts = True)\n",
    "#     v = len(uniq_words) # number of unique words\n",
    "#     v1 = len(np.where(counts==1)[0])\n",
    "    \n",
    "#     honores = (100*math.log(N))/(1-v1/v)\n",
    "#     return honores\n",
    "\n",
    "# print(calc_honores(sample_mem))\n",
    "# print(calc_honores(sample_tech))\n",
    "\n",
    "# print(calc_honores(\"I went to store then I went to the gym then I went to the house\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# #Brunet's W index\n",
    "# def calc_brunets(sample_string):\n",
    "#     split_string = re.sub('['+string.punctuation.replace('\\'','')+']',' ',sample_string).split()\n",
    "#     N = len(split_string) #number of words\n",
    "#     uniq_words, counts = np.unique(split_string, return_counts = True)\n",
    "#     v = len(uniq_words) # number of unique words\n",
    "#     brunets = N**(v-0.165)\n",
    "#     return brunets\n",
    "\n",
    "# print(calc_brunets(sample_mem))\n",
    "# print(calc_brunets(sample_tech))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Use Renee's Correlated Features to Build a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#https://raw.githubusercontent.com/dwyl/english-words/master/words.txt\n",
    "with open('words.txt','r') as words:\n",
    "    word_list = words.read().splitlines()\n",
    "\n",
    "word_list = [w.lower() for w in word_list]\n",
    "\n",
    "\n",
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def catch(func, handle=lambda e : e, *args, **kwargs):\n",
    "    try:\n",
    "        return func(*args, **kwargs)    \n",
    "    except ZeroDivisionError as z:\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        return handle(e)\n",
    "\n",
    "\n",
    "# ## Tokenization\n",
    "\n",
    "# In[69]:\n",
    "\n",
    "\n",
    "#Mis-spellings, word length\n",
    "#rm puctuation\n",
    "#keep contractions, posessives\n",
    "def list_tokenize(response):\n",
    "    return re.sub('['+string.punctuation.replace('\\'','')+']',' ',response).split()\n",
    "\n",
    "\n",
    "# ## Information extraction nltk\n",
    "\n",
    "# In[70]:\n",
    "\n",
    "\n",
    "#information-extraction\n",
    "#https://www.nltk.org/book/ch07.html\n",
    "def ie_preprocess(document):\n",
    "    #sentence segmentation\n",
    "    sentences = nltk.sent_tokenize(document) \n",
    "    #tokenization\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    #pos tagging\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences] \n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "# ## Uniq, V\n",
    "\n",
    "# In[71]:\n",
    "\n",
    "\n",
    "def uniq_words (response,token_list=None, counts_vector_return=False,uniq_set_return=False):\n",
    "    if token_list is None: token_list = list_tokenize(response)\n",
    "    if counts_vector_return and uniq_set_return: return np.unique(token_list,return_counts=True)\n",
    "    \n",
    "    if counts_vector_return and not uniq_set_return: return np.unique(token_list,return_counts=True)[1]\n",
    "    \n",
    "    if uniq_set_return and not counts_vector_return: return np.unique(token_list)\n",
    "    #typical case, investigating number of uniq responses\n",
    "    \n",
    "    return len(np.unique(token_list))\n",
    "        \n",
    "\n",
    "\n",
    "# ## V1 \n",
    "\n",
    "# In[72]:\n",
    "\n",
    "\n",
    "def single_appearances (response,counts=None):\n",
    "    if counts is None: counts = uniq_words(response,counts_vector_return=True)\n",
    "    return len(np.where(counts==1)[0])\n",
    "\n",
    "\n",
    "# ## N-grams\n",
    "\n",
    "# In[73]:\n",
    "\n",
    "\n",
    "#http://locallyoptimal.com/blog/2013/01/20/elegant-n-gram-generation-in-python/\n",
    "#input\n",
    "def find_bigrams(response, return_list=False):\n",
    "    input_list = list_tokenize(response)\n",
    "    bigrams = zip(input_list, input_list[1:])\n",
    "    if return_list: return [b for b in bigrams]\n",
    "    else: return bigrams\n",
    "\n",
    "\n",
    "# ## Word count\n",
    "\n",
    "# In[74]:\n",
    "\n",
    "\n",
    "def word_count(response):\n",
    "    return len(list_tokenize(response))\n",
    "\n",
    "\n",
    "# ## Sentence count\n",
    "\n",
    "# In[75]:\n",
    "\n",
    "\n",
    "def sentence_count(response):\n",
    "    return len(ie_preprocess(response))\n",
    "\n",
    "\n",
    "# ### POS tagging\n",
    "\n",
    "# In[76]:\n",
    "\n",
    "\n",
    "#information-extraction\n",
    "#https://www.nltk.org/book/ch07.html\n",
    "def ie_preprocess(document):\n",
    "    #sentence segmentation\n",
    "    sentences = nltk.sent_tokenize(document) \n",
    "    #tokenization\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    #pos tagging\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences] \n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "# ## Rates\n",
    "\n",
    "# In[77]:\n",
    "\n",
    "\n",
    "def pos_rate(response,pos_response=None,tag_set=None,denom_set=None,N=None,nn=False,pn=False,vb=False,proper=None):\n",
    "    \n",
    "    if pos_response is None: pos_response = ie_preprocess(response)\n",
    "        \n",
    "    \n",
    "    if tag_set is None:\n",
    "        #common nouns, excluding proper nouns\n",
    "        if nn: tag_set = ['NN','NNS']\n",
    "        elif vb: tag_set = ['VB','VBD','VBG','VBN','VBP','VBZ'] \n",
    "        elif pn: tag_set = ['PRP','PRP$']\n",
    "        elif proper: \n",
    "            tag_set = ['PRP','PRP$','WP','WP$']\n",
    "            denom_set = ['PRP','PRP$','WP','WP$','NN','NNP','NNPS','NNS']\n",
    "            \n",
    "        \n",
    "    pos_count = sum([1 for sentence in pos_response for word in sentence if word[1] in tag_set])\n",
    "    \n",
    "    if denom_set: \n",
    "        denom_count = sum([1 for sentence in pos_response for word in sentence if word[1] in denom_set])\n",
    "    else:\n",
    "        if N is None: N = word_count(response)\n",
    "        denom_count = N\n",
    "    \n",
    "    if (denom_count == 0):\n",
    "        return 0\n",
    "    else:\n",
    "        return pos_count/denom_count\n",
    "        \n",
    "\n",
    "#         sum([1 for sentence in pos_test for word in sentence if word[1] in ['VB','VBD','VBG','VBN','VBP','VBZ'] ])\n",
    "\n",
    "\n",
    "# ## Honore's R \n",
    "# ![image.png](attachment:image.png)\n",
    "# https://books.google.com/books?id=CwC4CwAAQBAJ&pg=PA111&lpg=PA111&dq=honore+statistic+lexical+richness&source=bl&ots=LPnM2j9oX5&sig=HGObkoMYRy6lLsgc80Y7tUMT7vk&hl=en&sa=X&ved=2ahUKEwjv0ubE7vTdAhXwYd8KHVRfCiMQ6AEwCXoECAUQAQ#v=onepage&q=honore&f=false\n",
    "# \n",
    "# http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.2.1359&rep=rep1&type=pdf\n",
    "# \n",
    "# http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.484.4648&rep=rep1&type=pdf\n",
    "# \n",
    "# viii) Honoré’s R statistic.\n",
    "# If V1 denotes the number of hapax legomena recorded, then Honoré (1979) defined his\n",
    "# statistic as:\n",
    "# \n",
    "# *R = 100log(N)/(1 - V1/V)*\n",
    "# \n",
    "# As a measure of vocabulary richness which has the virtue of being insensitive to text\n",
    "# length, it has been used in stylometric studies by Holmes (1992) and Holmes and\n",
    "# Stylometric analysis of conversational speech\n",
    "# Forsyth (1995). Values of R typically range from 1000 to 2000 with higher values\n",
    "# implying richer vocabularies in the sense that a greater number of words appear\n",
    "# infrequently.\n",
    "# \n",
    "# \n",
    "# https://pdfs.semanticscholar.org/11f9/ef33ad001f7638ba29ee8109077de92eb1bb.pdf\n",
    "# \n",
    "# \n",
    "# Assuming log == base e...fits with 1000-2000 assumption\n",
    "# \n",
    "# \n",
    "# \n",
    "\n",
    "# In[78]:\n",
    "\n",
    "\n",
    "def honore_r(response,v=None,v1=None, N=None):\n",
    "    if N is None: N = word_count(response)\n",
    "    if v is None: v = uniq_responses(response)\n",
    "    if v1 is None: \n",
    "        counts = uniq_responses(response,counts_vector_return=True)\n",
    "        v1 = len(np.where(counts==1)[0])\n",
    "    return (100*math.log(N))/(1-v1/v)\n",
    "\n",
    "\n",
    "# ## Brunet's W\n",
    "# W = N**(V −0.165)\n",
    "# \n",
    "# (vii) Brunet’s W index.\n",
    "# This index, devised by Brunet (1978) and used successfully by Holmes and Forsyth\n",
    "# (1995), is a measure of vocabulary richness which is insensitive to text length. It is\n",
    "# defined as:\n",
    "# \n",
    "# W= N^(V-0.165)\n",
    "# where N is the text length, V the number of different words and (-0.165) is a scaling\n",
    "# constant proposed by Brunet. The measure generally varies between 10 and 20 with a\n",
    "# low value indicating a lexically richer speech.\n",
    "# https://pdfs.semanticscholar.org/11f9/ef33ad001f7638ba29ee8109077de92eb1bb.pdf\n",
    "\n",
    "# In[79]:\n",
    "\n",
    "\n",
    "def brunet_w(response,N=None, v=None):\n",
    "    if N is None: N = word_count(response)\n",
    "    if v is None: v = uniq_responses(response)\n",
    "    return N**(v-0.165)\n",
    "\n",
    "\n",
    "# ## Moving window\n",
    "\n",
    "# In[80]:\n",
    "\n",
    "\n",
    "#10-window size generator\n",
    "#https://docs.python.org/release/2.3.5/lib/itertools-example.html\n",
    "def window(seq, n=10):\n",
    "    \"Returns a sliding window (of width n) over data from the iterable\"\n",
    "    \"   s -> (s0,s1,...s[n-1]), (s1,s2,...,sn), ...                   \"\n",
    "    it = iter(seq)\n",
    "    result = tuple(islice(it, n))\n",
    "    if len(result) == n:\n",
    "        yield result    \n",
    "    for elem in it:\n",
    "        result = result[1:] + (elem,)\n",
    "        yield result\n",
    "\n",
    "def coord_conjunctions_init(response):\n",
    "    return sum([1 for sentence in ie_preprocess(response) if sentence[0][1] == 'CC'])\n",
    "\n",
    "def misspelt(response):\n",
    "    return sum([1 for word in list_tokenize(response) if word.lower() not in word_list])\n",
    "\n",
    "def calc_MATTR(mr):\n",
    "    window_iterable = window(list_tokenize(mr),n=min(len(list_tokenize(mr)),10))\n",
    "    return np.mean([len(np.unique(mw))/min(len(list_tokenize(mr)),10) for mw in window_iterable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html\n",
    "# from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "# vader = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp 3.1: See how language features compare to non-lang for users with memory responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "have_mem_indices = all_data.index[all_data[memory_prompt] != \"\"] #Gets the indices of rows that have memory response\n",
    "print(have_mem_indices.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "have_mem_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mem_cft_scores = pd.Series(all_data.loc[have_mem_indices]['FinalScore'], index=have_mem_indices) #Will be used as target for both language and non-language experiments\n",
    "print(mem_cft_scores.shape)\n",
    "\n",
    "#replace NaN with mean\n",
    "mem_cft_median_score = mem_cft_scores.median()\n",
    "mem_cft_mean_score = mem_cft_scores.mean()\n",
    "mem_cft_scores.fillna(mem_cft_mean_score, inplace=True)\n",
    "\n",
    "\n",
    "### MAKE A THREE-CLASS TARGET, BASED ON CFT WEBSITE\n",
    "mem_THREE_class_cft = mem_cft_scores.copy()\n",
    "\n",
    "mem_THREE_class_cft = mem_THREE_class_cft.where(mem_THREE_class_cft > 38, 0)\n",
    "mem_THREE_class_cft = mem_THREE_class_cft.where(mem_THREE_class_cft < 43, 2)\n",
    "mem_THREE_class_cft = mem_THREE_class_cft.where(((mem_THREE_class_cft == 0) | (mem_THREE_class_cft == 2)), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hold Out Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "held_out_test_mem_idx = np.random.choice(have_mem_indices,(1,int(.20*len(have_mem_indices))),replace=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "array([['40712', '44816', '43211', '51913', '41880', '39575', '29112',\n",
    "        '54397', '29004', '52080', '44345', '57361', '29113', '49691',\n",
    "        '40020', '38603', '46323', '29136', '55057', '54236', '43644',\n",
    "        '47950', '29670', '55991', '41751', '30279', '35853', '58820',\n",
    "        '29615', '35337', '59682', '42780', '40118', '54884', '29056',\n",
    "        '32304', '51163', '43205', '32565', '29234', '53994', '58441',\n",
    "        '29084', '32601', '34097', '46015', '42587', '36881', '47461',\n",
    "        '49925', '39966', '54091', '43429', '29608', '43700', '45405',\n",
    "        '29563', '54740', '30384', '29768', '32182', '31723', '32871',\n",
    "        '50433', '40693', '39695', '46613', '41045', '40854', '38440',\n",
    "        '38649', '45884']], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# held_out_test_mem_idx = ['40712', '44816', '43211', '51913', '41880', '39575', '29112', '54397', '29004', '52080', '44345', '57361', '29113', '49691', '40020', '38603', '46323', '29136', '55057', '54236', '43644', '47950', '29670', '55991', '41751', '30279', '35853', '58820', '29615', '35337', '59682', '42780', '40118', '54884', '29056', '32304', '51163', '43205', '32565', '29234', '53994', '58441', '29084', '32601', '34097', '46015', '42587', '36881', '47461', '49925', '39966', '54091', '43429', '29608', '43700', '45405', '29563', '54740', '30384', '29768', '32182', '31723', '32871', '50433', '40693', '39695', '46613', '41045', '40854', '38440', '38649', '45884']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_mem_heldout = mem_THREE_class_cft[held_out_test_mem_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_mem_heldout.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dev_mem_ix = [m for m in have_mem_indices if m not in held_out_test_mem_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_mem_idx = np.random.choice(train_dev_mem_ix,(1,int(.20*len(train_dev_mem_ix))),replace=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dev_mem_idx = ['46047', '58728', '42364', '39539', '39862', '30087', '58354',\n",
    "#         '47480', '56239', '46765', '39880', '34753', '31795', '45647',\n",
    "#         '43233', '32846', '55823', '31075', '43830', '39573', '53678',\n",
    "#         '57834', '32144', '39171', '52174', '39582', '33976', '45821',\n",
    "#         '43155', '53050', '52053', '53125', '39082', '59103', '37078',\n",
    "#         '33444', '29051', '40017', '39830', '42876', '29970', '31871',\n",
    "#         '31482', '39319', '54924', '44439', '38414', '29502', '30865',\n",
    "#         '32233', '43247', '57709', '53472', '40938', '29129', '57407',\n",
    "#         '42040', '40253']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_mem_dev = mem_THREE_class_cft[dev_mem_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_mem_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_mem_idx =  [i for i in train_dev_mem_ix if i not in dev_mem_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(train_mem_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_mem_train = mem_THREE_class_cft[train_mem_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def prediction_accuracies(X_train, X_test, y_train, y_test):\n",
    "    ## Models to test\n",
    "    model_dict = {\n",
    "        'Naive Bayes': MultinomialNB(),\n",
    "        'Support Vector Machines': SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42,max_iter=5, tol=None),\n",
    "        'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "                                     random_state=0),\n",
    "        'Decision Tree': tree.DecisionTreeClassifier(),\n",
    "        \"3-Nearest-Neighbors\": KNeighborsClassifier(n_neighbors=3)\n",
    "    }\n",
    "\n",
    "    for model_name in model_dict:\n",
    "        model = model_dict[model_name]\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        predicted = model.predict(X_test)\n",
    "        predict_accuracy = np.mean(predicted == y_test)\n",
    "        \n",
    "        if (model_name == 'Naive Bayes'):\n",
    "            nb_predic_acc = predict_accuracy\n",
    "        if (model_name == 'Support Vector Machines'):\n",
    "            svm_predic_acc = predict_accuracy\n",
    "        if(model_name == 'Random Forest'):\n",
    "            rf_predic_acc = predict_accuracy\n",
    "        if(model_name == 'Decision Tree'):\n",
    "            dec_acc = predict_accuracy\n",
    "        if(model_name == \"3-Nearest-Neighbors\"):\n",
    "            knn_acc = predict_accuracy\n",
    "        print(model_name + \" Prediction Accuracy:\")\n",
    "        print(predict_accuracy)\n",
    "        \n",
    "        target_names = ['Significant Risk', 'At Risk', 'Normal']\n",
    "        \n",
    "        print(classification_report(y_test, predicted, target_names=target_names))\n",
    "    return nb_predic_acc, svm_predic_acc, rf_predic_acc, dec_acc, knn_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def prediction_accuracy(model_name, X_train, X_test, y_train, y_test):\n",
    "#     model_dict = {\n",
    "#         'Naive Bayes': MultinomialNB(),\n",
    "#         'Support Vector Machines': SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42,max_iter=5, tol=None),\n",
    "#         'Random Forest': RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "#                                      random_state=0),\n",
    "#         'Decision Tree': tree.DecisionTreeClassifier(),\n",
    "#         \"3-Nearest-Neighbors\": KNeighborsClassifier(n_neighbors=3)\n",
    "#     }\n",
    "\n",
    "#     model = model_dict[model_name]\n",
    "#     model.fit(X_train, y_train)\n",
    "\n",
    "#     predicted = model.predict(X_test)\n",
    "#     predict_accuracy = np.mean(predicted == y_test)\n",
    "\n",
    "#     return predict_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp 3.1, Part 1: Run models on Non-Language Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mem_responses_no_text = X.loc[have_mem_indices]\n",
    "mem_responses_no_text.drop('FinalScore', axis=1, inplace=True) #Those with memory respones: all data besides text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_text_mem_train = mem_responses_no_text.loc[train_mem_idx]\n",
    "\n",
    "no_text_mem_dev = mem_responses_no_text.loc[dev_mem_idx]\n",
    "\n",
    "nb_no_text_mem_acc, svm_no_text_mem_acc, rf_no_text_mem_acc, dec_tree_no_text_mem_acc, knn_no_text_mem_acc = prediction_accuracies( no_text_mem_train, no_text_mem_dev, y_mem_train, y_mem_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_text_mem_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for no_text_mem_col in no_text_mem_train.columns:\n",
    "    dropped_X_train = no_text_mem_train.drop(no_text_mem_col, axis=1)\n",
    "    dropped_X_dev = no_text_mem_dev.drop(no_text_mem_col, axis=1)\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "                                     random_state=0)\n",
    "    model.fit(dropped_X_train, y_mem_train)\n",
    "\n",
    "    predicted = model.predict(dropped_X_dev)\n",
    "    predict_accuracy = np.mean(predicted == y_mem_dev)\n",
    "    \n",
    "    if(predict_accuracy < rf_no_text_mem_acc):\n",
    "        print(no_text_mem_col)\n",
    "        print(predict_accuracy)\n",
    "        print(\"..........................................\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp 3.2, Part 2: Run Models on Language Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mem_responses = all_data.loc[have_mem_indices].iloc[:,memory_prompt_idx] #Just responses themselves\n",
    "print(mem_responses.shape)\n",
    "\n",
    "memory_response_df = mem_responses.to_frame(name='Response')\n",
    "memory_response_df['NumWords'] = memory_response_df.apply(lambda row: word_count(row['Response']), axis=1)\n",
    "memory_response_df['NumSentences'] = memory_response_df.apply(lambda row: sentence_count(row['Response'].decode('utf-8')), axis=1)\n",
    "memory_response_df['V'] = memory_response_df.apply(lambda row: uniq_words(row['Response']), axis=1)\n",
    "memory_response_df['V1'] = memory_response_df.apply(lambda row: single_appearances(row['Response']), axis=1)\n",
    "memory_response_df['MATTR'] = memory_response_df.apply(lambda row: calc_MATTR(row['Response']), axis=1)\n",
    "memory_response_df['CoordinatingConjunctions'] = memory_response_df.apply(lambda row: coord_conjunctions_init(row['Response'].decode('utf-8')), axis=1)\n",
    "memory_response_df['Misspellings'] = memory_response_df.apply(lambda row: misspelt(row['Response'].decode('utf-8')), axis=1)\n",
    "memory_response_df['NounRate'] = memory_response_df.apply(lambda row: pos_rate(row['Response'].decode('utf-8'),nn=True), axis=1)\n",
    "\n",
    "memory_response_df['VerbRate'] = memory_response_df.apply(lambda row: pos_rate(row['Response'].decode('utf-8'),vb=True), axis=1)\n",
    "\n",
    "memory_response_df['PronounRate'] = memory_response_df.apply(lambda row: pos_rate(row['Response'].decode('utf-8'),pn=True), axis=1) \n",
    "\n",
    "memory_response_df['ProperNounRate'] = memory_response_df.apply(lambda row: pos_rate(row['Response'].decode('utf-8'),proper=True), axis=1) \n",
    "\n",
    "memory_response_df.drop('Response', axis=1, inplace=True) #get rid of actual text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "memory_response_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_mem_train = memory_response_df.loc[train_mem_idx]\n",
    "\n",
    "text_mem_dev = memory_response_df.loc[dev_mem_idx]\n",
    "\n",
    "nb_text_mem_acc, svm_text_mem_acc, rf_text_mem_acc, dec_tree_text_mem_acc, knn_text_mem_acc = prediction_accuracies( text_mem_train, text_mem_dev, y_mem_train, y_mem_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp 3.2, Part 3:  Combine Text and No-Text and Test Same Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_mem = mem_responses_no_text.join(memory_response_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_mem_train = combined_mem.loc[train_mem_idx]\n",
    "combined_mem_dev = combined_mem.loc[dev_mem_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#combined_mem_train, combined_mem_test, combined_score_train, combined_score_test = train_test_split(combined_mem,mem_THREE_class_cft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_all_mem_acc, svm_all_mem_acc, rf_all_mem_acc, dec_all_text_mem_acc, knn_all_mem_acc = prediction_accuracies( combined_mem_train, combined_mem_dev, y_mem_train, y_mem_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Exp 3.2, Part 4:  Add Text Features one at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiment, we look only at the effect on Random Forest, which showed an improved prediction accuracy for JUST Lang data and Lang + Non-Lang data   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, mem_resp_col in enumerate(memory_response_df.columns):\n",
    "    feature_column = memory_response_df[mem_resp_col]\n",
    "    concatenated = mem_responses_no_text.join(feature_column)\n",
    "\n",
    "    X_train = concatenated.loc[train_mem_idx]\n",
    "    X_dev = concatenated.loc[dev_mem_idx]\n",
    "   \n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=5,\n",
    "                                     random_state=0)\n",
    "    model.fit(X_train, y_mem_train)\n",
    "\n",
    "    predicted = model.predict(X_dev)\n",
    "    predict_accuracy = np.mean(predicted == y_mem_dev)\n",
    "    \n",
    "    if (predict_accuracy > rf_no_text_mem_acc):\n",
    "        print(\"Feature Tested: \" + mem_resp_col)\n",
    "        print(predict_accuracy)\n",
    "        print(\"................................\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we take the best individually-performing feature additions and combine them with non-text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "good_mem_text_features = ['NumSentences', 'V', 'MATTR', 'Misspellings', 'VerbRate']\n",
    "\n",
    "combined_good = mem_responses_no_text.join(memory_response_df[good_mem_text_features])\n",
    "\n",
    "X_train = concatenated.loc[train_mem_idx]\n",
    "X_dev = concatenated.loc[dev_mem_idx]\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=5,\n",
    "                                 random_state=0)\n",
    "model.fit(X_train, y_mem_train)\n",
    "\n",
    "predicted = model.predict(X_dev)\n",
    "predict_accuracy = np.mean(predicted == y_mem_dev)\n",
    "\n",
    "print(predict_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exp 4: Do the same with Tech Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "have_tech_indices = all_data.index[all_data[tech_prompt] != \"\"] #Gets the indices of rows that have tech response\n",
    "print(have_tech_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tech_cft_scores = pd.Series(all_data.loc[have_tech_indices]['FinalScore'], index=have_tech_indices) #Will be used as target for both language and non-language experiments\n",
    "print(tech_cft_scores.shape)\n",
    "\n",
    "#replace NaN with mean\n",
    "tech_cft_median_score = tech_cft_scores.median()\n",
    "tech_cft_mean_score = tech_cft_scores.mean()\n",
    "tech_cft_scores.fillna(tech_cft_mean_score, inplace=True)\n",
    "\n",
    "\n",
    "### MAKE A THREE-CLASS TARGET, BASED ON CFT WEBSITE\n",
    "tech_THREE_class_cft = tech_cft_scores.copy()\n",
    "\n",
    "tech_THREE_class_cft = tech_THREE_class_cft.where(tech_THREE_class_cft > 38, 0)\n",
    "tech_THREE_class_cft = tech_THREE_class_cft.where(tech_THREE_class_cft < 43, 2)\n",
    "tech_THREE_class_cft = tech_THREE_class_cft.where(((tech_THREE_class_cft == 0) | (tech_THREE_class_cft == 2)), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hold Out Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "held_out_test_tech_idx = np.random.choice(have_tech_indices,(1,int(.20*len(have_tech_indices))),replace=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# held_out_test_tech_idx = ['38874', '44404', '29421', '38698', '40693', '49414', '41751',\n",
    "#         '56026', '41612', '51136', '38611', '56084', '45787', '56784',\n",
    "#         '39171', '53312', '47004', '56291', '49527', '38604', '40084',\n",
    "#         '31837', '45294', '55091', '54529', '59200', '28867', '50329',\n",
    "#         '29733', '38820', '35401', '28926', '42044', '40020', '55165',\n",
    "#         '54881', '39399', '48595', '54884', '48604', '57444', '55417',\n",
    "#         '31120', '43247', '43004', '39012', '31795', '30801', '29079',\n",
    "#         '39582', '58441', '53994', '46323', '55642', '40227', '49925',\n",
    "#         '30279', '39452', '43624', '45993', '31871', '43507', '39221',\n",
    "#         '29088', '41000', '53125', '59703', '38700', '40189', '43429',\n",
    "#         '32565', '57407', '44151', '59243', '39880', '29258', '45700',\n",
    "#         '37300', '41703', '31008', '30762', '28932', '39942', '35914']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_tech_heldout = tech_THREE_class_cft[held_out_test_tech_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_tech_heldout.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dev Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dev_tech_ix = [m for m in have_tech_indices if m not in held_out_test_tech_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_tech_idx = np.random.choice(train_dev_tech_ix,(1,int(.20*len(train_dev_tech_ix))),replace=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dev_tech_idx = ['40854', '44234', '51913', '41997', '29059', '45772', '40595',\n",
    "#         '52023', '48606', '38440', '51713', '57709', '39539', '46170',\n",
    "#         '30642', '32846', '31945', '43248', '34753', '57299', '46047',\n",
    "#         '51163', '30522', '38686', '43155', '34515', '38754', '44730',\n",
    "#         '29073', '40732', '60026', '41626', '59232', '44439', '46055',\n",
    "#         '38603', '30865', '38492', '29090', '38830', '39830', '45647',\n",
    "#         '57812', '43830', '42443', '56818', '39901', '52767', '38944',\n",
    "#         '46166', '29333', '54101', '29886', '55823', '48498', '32233',\n",
    "#         '56239', '44235', '38803', '57834', '29805', '47438', '40530',\n",
    "#         '56110', '38538', '54924', '45821', '53050']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_tech_dev = tech_THREE_class_cft[dev_tech_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_tech_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_tech_idx =  [i for i in train_dev_tech_ix if i not in dev_tech_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_tech_train = tech_THREE_class_cft[train_tech_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp 4.1, Part 1: Run models on Non-Language Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tech_responses_no_text = X.loc[have_tech_indices]\n",
    "tech_responses_no_text.drop('FinalScore', axis=1, inplace=True) #Those with tech respones: all data besides text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_text_tech_train = tech_responses_no_text.loc[train_tech_idx]\n",
    "\n",
    "no_text_tech_dev = tech_responses_no_text.loc[dev_tech_idx]\n",
    "\n",
    "nb_no_text_tech_acc, svm_no_text_tech_acc, rf_no_text_tech_acc, dec_tree_no_text_tech_acc, knn_no_text_tech_acc = prediction_accuracies( no_text_tech_train, no_text_tech_dev, y_tech_train, y_tech_dev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Exp 4.1, Part 2: Run models on Language Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tech_responses = all_data.loc[have_tech_indices].iloc[:,tech_prompt_idx] #Just responses themselves\n",
    "print(tech_responses.shape)\n",
    "\n",
    "tech_response_df = tech_responses.to_frame(name='Response')\n",
    "tech_response_df['NumWords'] = tech_response_df.apply(lambda row: word_count(row['Response']), axis=1)\n",
    "tech_response_df['NumSentences'] = tech_response_df.apply(lambda row: sentence_count(row['Response'].decode('utf-8')), axis=1)\n",
    "tech_response_df['V'] = tech_response_df.apply(lambda row: uniq_words(row['Response']), axis=1)\n",
    "tech_response_df['V1'] = tech_response_df.apply(lambda row: single_appearances(row['Response']), axis=1)\n",
    "tech_response_df['MATTR'] = tech_response_df.apply(lambda row: calc_MATTR(row['Response']), axis=1)\n",
    "tech_response_df['CoordinatingConjunctions'] = tech_response_df.apply(lambda row: coord_conjunctions_init(row['Response'].decode('utf-8')), axis=1)\n",
    "tech_response_df['Misspellings'] = tech_response_df.apply(lambda row: misspelt(row['Response'].decode('utf-8')), axis=1)\n",
    "tech_response_df['NounRate'] = tech_response_df.apply(lambda row: pos_rate(row['Response'].decode('utf-8'),nn=True), axis=1)\n",
    "\n",
    "tech_response_df['VerbRate'] = tech_response_df.apply(lambda row: pos_rate(row['Response'].decode('utf-8'),vb=True), axis=1)\n",
    "\n",
    "tech_response_df['PronounRate'] = tech_response_df.apply(lambda row: pos_rate(row['Response'].decode('utf-8'),pn=True), axis=1) \n",
    "\n",
    "tech_response_df['ProperNounRate'] = tech_response_df.apply(lambda row: pos_rate(row['Response'].decode('utf-8'),proper=True), axis=1) \n",
    "\n",
    "tech_response_df.drop('Response', axis=1, inplace=True) #get rid of actual text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_tech_train = tech_response_df.loc[train_tech_idx]\n",
    "\n",
    "text_tech_dev = tech_response_df.loc[dev_tech_idx]\n",
    "\n",
    "nb_text_tech_acc, svm_text_tech_acc, rf_text_tech_acc, dec_tree_text_tech_acc, knn_text_tech_acc = prediction_accuracies( text_tech_train, text_tech_dev, y_tech_train, y_tech_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exp 4.2, Part 3:  Combine Text and No-Text and Test Same Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_tech = tech_responses_no_text.join(tech_response_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_tech_train = combined_tech.loc[train_tech_idx]\n",
    "combined_tech_dev = combined_tech.loc[dev_tech_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_all_tech_acc, svm_all_tech_acc, rf_all_tech_acc, dec_all_text_tech_acc, knn_all_tech_acc = prediction_accuracies( combined_tech_train, combined_tech_dev, y_tech_train, y_tech_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Exp 4.2, Part 4:  Add Text Features one at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this experiment, we look only at the effect on Random Forest, which showed an improved prediction accuracy for JUST Lang data and Lang + Non-Lang data   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for idx, tech_resp_col in enumerate(tech_response_df.columns):\n",
    "    feature_column = tech_response_df[tech_resp_col]\n",
    "    concatenated = tech_responses_no_text.join(feature_column)\n",
    "\n",
    "    X_train = concatenated.loc[train_tech_idx]\n",
    "    X_dev = concatenated.loc[dev_tech_idx]\n",
    "   \n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=5,\n",
    "                                     random_state=0)\n",
    "    model.fit(X_train, y_tech_train)\n",
    "\n",
    "    predicted = model.predict(X_dev)\n",
    "    predict_accuracy = np.mean(predicted == y_tech_dev)\n",
    "    \n",
    "    #if (predict_accuracy > rf_no_text_tech_acc):\n",
    "    print(\"Feature Tested: \" + tech_resp_col)\n",
    "    print(predict_accuracy)\n",
    "    print(\"................................\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# FINAL EXPERIMENTS USING HELD-OUT TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory, no Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "no_text_mem_train = mem_responses_no_text.loc[train_mem_idx]\n",
    "\n",
    "no_text_mem_test = mem_responses_no_text.loc[held_out_test_mem_idx]\n",
    "\n",
    "final_nb_no_text_mem_acc, final_svm_no_text_mem_acc, final_rf_no_text_mem_acc, final_dec_tree_no_text_mem_acc, final_knn_no_text_mem_acc = prediction_accuracies( no_text_mem_train, no_text_mem_test, y_mem_train, y_mem_heldout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#having trouble getting this from the classification report so writing this down from printed results for now\n",
    "mem_no_text_precision = (20, 0, 0, 12, 0) #nb,svm, rf, dt,knn\n",
    "mem_no_text_recall = (22, 0, 0, 11, 0)\n",
    "mem_no_text_f1 = (21, 0, 0, 12, 0)\n",
    "\n",
    "\n",
    "# mem_no_text_knn_precision, mem_no_text_knn_recall, mem_no_text_knn_f1 = 51, 60, 55\n",
    "# mem_no_text_svm_precision, mem_no_text_svm_recall, mem_no_text_svm_f1 = 46,67,54\n",
    "# mem_no_text_nb_precision, mem_no_text_nb_recall, mem_no_text_knn_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for no_text_mem_col in no_text_mem_train.columns:\n",
    "    dropped_X_train = no_text_mem_train.drop(no_text_mem_col, axis=1)\n",
    "    dropped_X_test = no_text_mem_test.drop(no_text_mem_col, axis=1)\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "                                     random_state=0)\n",
    "    model.fit(dropped_X_train, y_mem_train)\n",
    "\n",
    "    predicted = model.predict(dropped_X_test)\n",
    "    predict_accuracy = np.mean(predicted == y_mem_heldout)\n",
    "    \n",
    "    if(predict_accuracy < final_rf_no_text_mem_acc):\n",
    "        print(no_text_mem_col)\n",
    "        print(predict_accuracy)\n",
    "        print(\"..........................................\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory, Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_mem_train = memory_response_df.loc[train_mem_idx]\n",
    "\n",
    "text_mem_test = memory_response_df.loc[held_out_test_mem_idx]\n",
    "\n",
    "final_nb_text_mem_acc, final_svm_text_mem_acc, final_rf_text_mem_acc, final_dec_tree_text_mem_acc, final_knn_text_mem_acc = prediction_accuracies( text_mem_train, text_mem_test, y_mem_train, y_mem_heldout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem_text_precision = (0, 0, 0, 13, 10)\n",
    "mem_text_recall = (0, 0, 0, 22, 11)\n",
    "mem_text_f1 = (0, 0, 0, 17, 11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for text_mem_col in text_mem_train.columns:\n",
    "    dropped_X_train = text_mem_train.drop(text_mem_col, axis=1)\n",
    "    dropped_X_test = text_mem_test.drop(text_mem_col, axis=1)\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "                                     random_state=0)\n",
    "    model.fit(dropped_X_train, y_mem_train)\n",
    "\n",
    "    predicted = model.predict(dropped_X_test)\n",
    "    predict_accuracy = np.mean(predicted == y_mem_heldout)\n",
    "    \n",
    "    if(predict_accuracy < final_rf_text_mem_acc):\n",
    "        print(text_mem_col)\n",
    "        print(predict_accuracy)\n",
    "        print(\"..........................................\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory, Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_mem = mem_responses_no_text.join(memory_response_df)\n",
    "\n",
    "combined_mem_train = combined_mem.loc[train_mem_idx]\n",
    "combined_mem_test = combined_mem.loc[held_out_test_mem_idx]\n",
    "\n",
    "final_nb_all_mem_acc, final_svm_all_mem_acc, final_rf_all_mem_acc, final_dec_all_text_mem_acc, final_knn_all_mem_acc = prediction_accuracies( combined_mem_train, combined_mem_test, y_mem_train, y_mem_heldout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mem_combined_precision = (18, 0, 0, 17, 0)\n",
    "mem_combined_recall = (22, 0, 0, 24, 0)\n",
    "mem_combined_f1 = (20, 0, 0, 13, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for idx, mem_resp_col in enumerate(combined_mem.columns):\n",
    "#     feature_column = combined_mem[mem_resp_col]\n",
    "#     concatenated = mem_responses_no_text.join(feature_column)\n",
    "\n",
    "#     X_train = concatenated.loc[train_mem_idx]\n",
    "#     X_test = concatenated.loc[held_out_test_mem_idx]\n",
    "   \n",
    "#     model = tree.DecisionTreeClassifier()\n",
    "#     model.fit(X_train, y_mem_train)\n",
    "\n",
    "#     predicted = model.predict(X_test)\n",
    "#     predict_accuracy = np.mean(predicted == y_mem_heldout)\n",
    "    \n",
    "#     if (predict_accuracy > final_rf_all_mem_acc):\n",
    "#         print(\"Feature Tested: \" + mem_resp_col)\n",
    "#         print(predict_accuracy)\n",
    "#         print(\"................................\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tech, No Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "no_text_tech_train = tech_responses_no_text.loc[train_tech_idx]\n",
    "\n",
    "no_text_tech_test = tech_responses_no_text.loc[held_out_test_tech_idx]\n",
    "\n",
    "final_nb_no_text_tech_acc, final_svm_no_text_tech_acc, final_rf_no_text_tech_acc, final_dec_tree_no_text_tech_acc, final_knn_no_text_tech_acc = prediction_accuracies( no_text_tech_train, no_text_tech_test, y_tech_train, y_tech_heldout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_no_text_precision = (18, 0, 0, 60, 20)\n",
    "tech_no_text_recall = (22, 0, 0, 33, 11)\n",
    "tech_no_text_f1 = (20, 4, 0, 0, 43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for no_text_tech_col in no_text_tech_train.columns:\n",
    "    dropped_X_train = no_text_tech_train.drop(no_text_tech_col, axis=1)\n",
    "    dropped_X_test = no_text_tech_test.drop(no_text_tech_col, axis=1)\n",
    "    \n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "                                     random_state=0)\n",
    "    model.fit(dropped_X_train, y_tech_train)\n",
    "\n",
    "    predicted = model.predict(dropped_X_test)\n",
    "    predict_accuracy = np.mean(predicted == y_tech_heldout)\n",
    "    \n",
    "    if(predict_accuracy < final_rf_no_text_tech_acc):\n",
    "        print(no_text_tech_col)\n",
    "        print(predict_accuracy)\n",
    "        print(\"..........................................\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tech, Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_tech_train = tech_response_df.loc[train_tech_idx]\n",
    "\n",
    "text_tech_test = tech_response_df.loc[held_out_test_tech_idx]\n",
    "\n",
    "final_nb_text_tech_acc, final_svm_text_tech_acc, final_rf_text_tech_acc, final_dec_tree_text_tech_acc, final_knn_text_tech_acc = prediction_accuracies(text_tech_train, text_tech_test, y_tech_train, y_tech_heldout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_text_precision = (0, 0, 0, 8, 20)\n",
    "tech_text_recall = (0, 0, 0, 11, 11)\n",
    "tech_text_f1 = (0,0,0,10,14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tech, Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_tech = tech_responses_no_text.join(tech_response_df)\n",
    "\n",
    "combined_tech_train = combined_tech.loc[train_tech_idx]\n",
    "combined_tech_test = combined_tech.loc[held_out_test_tech_idx]\n",
    "\n",
    "final_nb_all_tech_acc, final_svm_all_tech_acc, final_rf_all_tech_acc, final_dec_all_text_tech_acc, final_knn_all_tech_acc = prediction_accuracies( combined_tech_train, combined_tech_test, y_tech_train, y_tech_heldout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tech_combined_precision = (25, 29, 0, 64, 20)\n",
    "tech_combined_recall = (22, 22, 0, 44, 11)\n",
    "tech_combined_f1 = (24, 25, 0, 44, 14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RESULTS FIGURES SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "N = 3\n",
    "\n",
    "act_sig_risk = mem_THREE_class_cft[mem_THREE_class_cft == 0].shape[0] + tech_THREE_class_cft[tech_THREE_class_cft == 0].shape[0]\n",
    "act_at_risk = mem_THREE_class_cft[mem_THREE_class_cft == 1].shape[0] + tech_THREE_class_cft[tech_THREE_class_cft == 1].shape[0]\n",
    "act_normal = mem_THREE_class_cft[mem_THREE_class_cft == 2].shape[0] + tech_THREE_class_cft[tech_THREE_class_cft == 2].shape[0]\n",
    "\n",
    "total_cft = mem_THREE_class_cft.shape[0] + tech_THREE_class_cft.shape[0]\n",
    "\n",
    "objects = ('Significant Risk (<38)', 'At Risk (<43, >38)', 'Normal (>43)')\n",
    "\n",
    "mem_CFT = ((act_sig_risk/total_cft)*100, (act_at_risk/total_cft)*100, (act_normal/total_cft)*100)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ind = np.arange(N)    # the x locations for the groups\n",
    "width = 0.35         # the width of the bars\n",
    "p1 = ax.bar(ind, mem_CFT, width, color='r')\n",
    "\n",
    "\n",
    "expected_CFT = (6.7, 9.2, 84.1)\n",
    "p2 = ax.bar(ind + width, expected_CFT, width,\n",
    "            color='y')\n",
    "\n",
    "ax.set_title('Expected CFT Score Distribution and Actual Distribution')\n",
    "ax.set_xticks(ind + width / 2)\n",
    "ax.set_xticklabels(('Significant Risk (<38)', 'At Risk (38-43)', 'Normal (>43)'))\n",
    "\n",
    "ax.legend((p1[0], p2[0]), ('CFT Distribution in Dataset', 'Expected CFT Distribution'))\n",
    "ax.autoscale_view()\n",
    "\n",
    "plt.ylabel('% of Total')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Num_Models = 5\n",
    "\n",
    "mem_no_text = (final_nb_no_text_mem_acc*100, final_svm_no_text_mem_acc*100, final_rf_no_text_mem_acc*100, final_dec_tree_no_text_mem_acc*100, final_knn_no_text_mem_acc*100)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ind = np.arange(Num_Models)    # the x locations for the groups\n",
    "width = 0.25         # the width of the bars\n",
    "p1 = ax.bar(ind, mem_no_text, width, color='b')\n",
    "\n",
    "mem_text = (final_nb_text_mem_acc*100, final_svm_text_mem_acc*100, final_rf_text_mem_acc*100, final_dec_tree_text_mem_acc*100, final_knn_text_mem_acc*100)\n",
    "p2 = ax.bar(ind + width, mem_text, width,\n",
    "            color='g')\n",
    "\n",
    "mem_combined = (final_nb_all_mem_acc*100, final_svm_all_mem_acc*100, final_rf_all_mem_acc*100, final_dec_all_text_mem_acc*100, final_knn_all_mem_acc*100)\n",
    "p3 = ax.bar(ind + width + width, mem_combined, width,\n",
    "            color='#00FFFF')\n",
    "\n",
    "ax.set_title('Model Accuracy on \"Memory Users\"')\n",
    "ax.set_xticks(ind + width / 2)\n",
    "ax.set_xticklabels(('NB', 'SVM', 'RF', 'DecTree', 'kNN'))\n",
    "\n",
    "ax.legend((p1[0], p2[0], p3[0]), ('Non-Text Data', 'Text Data', 'Combined'), prop={'size': 6})\n",
    "ax.autoscale_view()\n",
    "ax.set_ylim(0,100)\n",
    "plt.ylabel('Prediction Accuracy (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Num_Models = 5\n",
    "\n",
    "tech_no_text = (final_nb_no_text_tech_acc*100, final_svm_no_text_tech_acc*100, final_rf_no_text_tech_acc*100, final_dec_tree_no_text_tech_acc*100, final_knn_no_text_tech_acc*100)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ind = np.arange(Num_Models)    # the x locations for the groups\n",
    "width = 0.25         # the width of the bars\n",
    "p1 = ax.bar(ind, tech_no_text, width, color='b')\n",
    "\n",
    "tech_text = (final_nb_text_tech_acc*100, final_svm_text_tech_acc*100, final_rf_text_tech_acc*100, final_dec_tree_text_tech_acc*100, final_knn_text_tech_acc*100)\n",
    "p2 = ax.bar(ind + width, tech_text, width,\n",
    "            color='g')\n",
    "\n",
    "\n",
    "tech_combined = (final_nb_all_tech_acc*100, final_svm_all_tech_acc*100, final_rf_all_tech_acc*100, final_dec_all_text_tech_acc*100, final_knn_all_tech_acc*100)\n",
    "p3 = ax.bar(ind + width + width, tech_combined, width,\n",
    "            color='#00FFFF')\n",
    "\n",
    "ax.set_title('Model Accuracy on \"Opinion Users\"')\n",
    "ax.set_xticks(ind + width / 2)\n",
    "ax.set_xticklabels(('NB', 'SVM', 'RF', 'DecTree', 'kNN'))\n",
    "\n",
    "ax.legend((p1[0], p2[0], p3[0]), ('Non-Text Data', 'Text Data', 'Combined'), prop={'size': 6})\n",
    "ax.set_ylim(0,100)\n",
    "ax.autoscale_view()\n",
    "\n",
    "plt.ylabel('Prediction Accuracy (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_bar_chart(non_tuple, text_tuple, combined_tuple, metric_string, user_type):\n",
    "    Num_Models = 5\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ind = np.arange(Num_Models)    # the x locations for the groups\n",
    "    width = 0.25         # the width of the bars\n",
    "    p1 = ax.bar(ind, non_tuple, width, color='b')\n",
    "\n",
    "    p2 = ax.bar(ind + width, text_tuple, width,\n",
    "                color='g')\n",
    "\n",
    "    p3 = ax.bar(ind + width + width, combined_tuple, width,\n",
    "                color='#00FFFF')\n",
    "\n",
    "    ax.set_title('Model ' + metric_string + ' on \"' + user_type + ' Users\"')\n",
    "    ax.set_xticks(ind + width / 2)\n",
    "    ax.set_xticklabels(('NB', 'SVM', 'RF', 'DecTree', 'kNN'))\n",
    "\n",
    "    ax.legend((p1[0], p2[0], p3[0]), ('Non-Text Data', 'Text Data', 'Combined'), prop={'size': 6})\n",
    "    ax.set_ylim(0, 100)\n",
    "    ax.autoscale_view()\n",
    "\n",
    "    plt.ylabel('Prediction ' + metric_string +  '(%)')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_bar_chart(mem_no_text_precision, mem_text_precision, mem_combined_precision, \"Precision\", \"Memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_bar_chart(tech_no_text_precision, tech_text_precision, tech_combined_precision, \"Precision\", \"Opinion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_bar_chart(mem_no_text_recall, mem_text_recall, mem_combined_recall, \"Recall\", \"Memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_bar_chart(tech_no_text_recall, tech_text_recall, tech_combined_recall, \"Recall\", \"Opinion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_bar_chart(mem_no_text_f1, mem_text_f1, mem_combined_f1, \"F1\", \"Memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "make_bar_chart(tech_no_text_f1, tech_text_f1, tech_combined_f1, \"F1\", \"Opinion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
